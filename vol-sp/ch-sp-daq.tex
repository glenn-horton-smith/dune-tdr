\chapter{Data Acquisition}
\label{ch:sp-daq}

\fixme{Fix SI units throughout, and remove a lot of the dword
  references (looks distracting). Georgia}

\section{Introduction}
\label{sec:fd-daq:introduction}

The \dword{fd} \dword{daq} system is responsible for receiving,
processing, and recording data from the \dword{dune} \dword{fd}. In doing so, it provides
timing and synchronization for all \dwords{detmodule} and
subdetectors; receives, synchronizes, compresses, and buffers streaming
data from the subdetectors; extracts information from the data at a
local level to subsequently make local, module, and cross-module data
selection decisions; builds event records %``events''
 from selected space-time volumes 
and relays them to permanent storage; and carries out local data
reduction and filtering of the data as needed.

This chapter provides a description for the design of the \dword{dune}
\dword{fd} \dword{daq} system developed by the \dword{dune} \dword{fd}
\dword{daq} consortium. 
This consortium brings together resources and expertise from CERN,
Colombia, France, Japan, the Netherlands, the UK, and the USA. 
Its members bring considerable experience from ICARUS, MicroBooNE,
SBND, and
DUNE prototype LArTPCs, as well as from ATLAS at the LHC and other major
HEP experiments across the world.

The system is designed %so that it 
to service all \dword{fd}
\dword{detmodule} designs indistinguishably. % different \dwords{detmodule}. 
%The descriptions for those parts of the system are reproduced verbatim in some sections of this chapter as found in the TDR volume for each \dword{detmodule}.
%A small part 
However, some aspects of the \dword{daq} design are tailored to meet
module-specific requirements, and those are documented in sections of this chapter which are 
unique to the \dword{detmodule} covered in this TDR volume;  
these sections are %identified 
identifiable by their use of module-specific terms. In general, the
DAQ services each \dword{fd}
\dword{detmodule} independently, but cross-module communication is
facilitated at the trigger level.

The chapter begins with an overview of the \dword{daq} design,
including requirements that the design must meet, and specification of
interfaces between the \dword{daq}  and other \dword{dune} \dword{fd} systems. 
Subsequently, Section~\ref{sec:fd-daq:design}, which comprises the
bulk of this chapter, describes the design of the \dword{fd}
\dword{daq} in greater detail.
Section~\ref{sec:fd-daq:protodune} describes design validation efforts
to date, and future design development and validation plans. At the
center of these efforts is the 
\dword{protodune} \dword{daq} system, which has served as a demonstrator of several
key aspects of the  \dword{dune}  \dword{daq}  design, and continues to serve as a
platform for further design development and validation. 
Finally, the chapter finishes with two sections providing details on
the management of the
\dword{daq} project, including schedule to completion of the design, 
production, and installation of the system, as well as cost, resources, and
safety considerations.

\section{Design Overview}
\label{sec:fd-daq:overview}

An overview of the \dword{dune} \dword{fd} \dword{daq} system 
servicing a single \dword{fd}
\dword{detmodule} is
provided in Fig.~\ref{fig:fd-daq:layout}. The system is
physically located at the \dword{fd} site, and it is split between the
4850 ft level and the ground level at SURF. Specifically, it occupies space and
power both in the central utility cavern (CUC) and the on-surface DAQ
room.  The front-end part of the system, which is responsible for
raw detector data reception and pre-processing, lives
underground in the CUC, while the back-end part of the system, which is responsible for
event-building as well as run control and monitoring, lives on the
surface. Data flows through the DAQ from the
front-end to the back-end of the system and to offline. The majority
of raw data processing and buffering is performed underground, in the
front-end part of the system, thus minimizing data bandwidth to the surface. A
hierarchical data selection subsystem consumes minimally-processed
information from the front-end readout, and
constructs module-level trigger decisions. Upon such decision, a data
flow orchestrator process is activated as part of the back-end part of
the system
to retrieve data to be built as part of an event record. At event
building stage, optional down-selection of the data is possible via
high-level filtering, prior to shipping the data to offline.  The
specifics of design implementation and data flow are described in Section~\ref{sec:fd-daq:design}.

\fixme{Figure should perhaps be modified to indicate low level data
  selection. Some rewording on boxes is also neeeded to match subsystem definitions. Georgia}

\begin{dunefigure}{fig:fd-daq:layout}{\dword{daq} design physical
    layout focusing on a single \SI{10}{\kilo\tonne} module. Not shown
    in this figure are the system control paths. 
  }
  \includegraphics[width=0.8\textwidth]{daq-layout.pdf}
\end{dunefigure}


\subsection{Requirements and Specifications}
\label{sec:fd-daq:requirements}

The \dword{dune} \dword{fd} \dword{daq} system is designed to meet the
\dword{dune} top-level as well as \dword{daq}-level requirements
summarized in Table~\ref{tab:specs:SP-DAQ}. The \dword{daq}-level requirements are
imposed to ensure that the 
system %is capable of 
can record all necessary information for offline 
analysis of data that is associated with on- and off-beam physics events, as directed
by the \dword{dune} physics mission, and with minimal compromise to
\dword{dune}'s physics sensitivity. The requirements must be met by following the 
specifications provided in the same table. Those specifications are
associated with trigger functionality, readout considerations,
and operations considerations, and are motivated further in the following subsections.

\subsubsection{How DUNE's Physics Mission Drives the DAQ Design}

The DUNE Far Detector has three main physics drivers: neutrino \dword{cpv} and related
long baseline oscillation studies using the high intensity beam provided
by \fnal, off-beam measurements of atmospheric neutrinos and searches
for rare processes such baryon-number-violating decays,
and detection of \dfirst{snb} occurring within our galaxy. The
\dword{dune} \dword{fd} \dword{daq} system must facilitate data
readout for delivering on these main physics drivers, while keeping
within physical (space, power) and resource constraints for
the system. In particular the off-beam measurements require the
continuous readout of the detector, and the lack of external triggers for such
events requires real-time or online data processing, and
self-triggering capabilities. Since the
continuous data rate of the far detector module reaches multiple
terabytes per second, significant data buffering and processing
resources are needed as part of the design.

The \dword{dune} \dword{fd} modules employ two
active detector components from which the the DAQ system must acquire data: \dfirst{tpc} and \dfirst{pds}. The two components access the physics %each 
by sensing and collecting signals associated with very different 
sensing time scales.
Ionization charge measurement by the \dword{tpc} for any given
localized activity in the detector requires a
nominal recording of data over a time window of order 
\SIrange{1}{10}{\milli\second}. 
This time scale is determined by the ionization electron drift speed in
\lar and the detector dimension along the drift direction.
On the other hand, the \dword{pds} measures argon scintillation light emission, which
occurs and is detected over a timescale of multiple nanoseconds to
microseconds for
any given event and/or subsequent subevent process. Unlike the TPC,
the PDS data is zero-suppressed in
the PDS electronics (see Chapter~\ref{}); therefore the total raw data volume received by
the DAQ system is be dominated by
the TPC data, which is sent out as a continuous stream.
 
Figure \ref{fig:daq-rates} provides the expected activity rates in a
single far detector module as a function of true energy associated
with given types of signal.
At low energy ($<$10 MeV), activity is dominated by radiological backgrounds
intrinsic to the detector, and
low-energy solar neutrino interactions. Supernova burst neutrinos are
would span the 10-30 MeV range, while at higher energies (generally
above 100 MeV), rates are dominated by cosmic rays, beam neutrino interactions,
and atmospheric neutrino interactions. With the exception of supernova
burst neutrinos, the activity associated with any of these physics
signals is localized in space and particularly in time. Supernova burst
neutrinos on the other hand are characteristically different, as they arrive as multiple
signals of localized activity that extensd over the entirety of the
detector and over multiple seconds. 

The nature and rates of these signatures necessitates a data selection strategy which handles two
distinct cases: a localized high energy activity trigger, prompting an event record readout for
activity associated with a minimum of 100 MeV of deposited energy; and an extended
low-energy activity trigger, prompting an event record readout when
multiple localized low energy activity candidates with a minimum of
10 MeV of deposited energy each are found over a short (less than 10
seconds) time period and over the entirety of a 10 kton
module. Because of the high granularity of the detector readout elements, a
hierarchical data selection subsystem is employed to provide data processing and
triggering, and facilitate optional data reduction and filtering. The
DAQ  
system is required to yield >99\% efficiency for localized high-
energy activity triggers, and sufficient efficiency for low-energy
activity trigger candidates as needed to achieve >90\% galactic
supernova burst trigger coverage. The galactic coverage is defined as
supernova burst trigger efficiency weighted supernova burst probability.

By offline considerations, the steady state rate of localized triggers
from the entire far detector is limited to 0.1 Hz, otherwise
more than 30 PB of data (uncompressed) would be generated per
year. This assumes (conservatively) that each localized
trigger prompts 5.4~ms of losslessly compressed TPC data plus PDS data from
the entire module to be read out as part of the event record. The average rate
of extended triggers is limited to 1 per month, per similar
considerations; this assumes that an extended trigger prompts 100 s of losslessly
compressed data from the entire module to be read out as part of the
event record. The capability of recording data losslessly is built
into the design as a conservative measure; a particular concern is
charge collection efficiency in the case of zero
suppression. MicroBooNE is currently investigating the impact of zero
suppression on reconstruction efficiency and energy resolution for
low-energy events. Expected
data rates from physics signals of interest, which fit the 30 PB
yearly generated volume and
trigger rate requirements, are summarized in Table~\ref{tab:sp-daq:rates}.

\begin{dunefigure}{fig:daq-rates}{Expected physics-related activity
    rates in a single \SI{10}{\kilo\tonne} module. \label{sec:fd-daq:rates}
}
  \includegraphics[width=0.6\textwidth]{daq-rates.png}
\end{dunefigure}

\begin{dunetable}
[Expected DAQ Yearly Data Rates]
{p{0.25\textwidth}p{0.1\textwidth}p{0.5\textwidth}}
{tab:sp-daq:rates}{Summary
    of expected data rates. The rates assume no compression, and are
    given for a single \SI{10}{\kilo\tonne} module. Trigger primitives
  are not kept permanently; they are temporarily stored for 1-2 months.}
Source  & Annual Data Volume & Assumptions \\\toprowrule
Beam interactions & 27 TB & 10 MeV threshold in coincidence with beam
time, including cosmic coincidence; 5.4 ms readout \\\colhline
Cosmics and atmospheric neutrinos & 10 PB & 5.4 ms readout \\\colhline
Radiological backgrounds & $<1$ PB & $<1$ per month fake rate for SNB
trigger\\\colhline
Cold Electronics calibration & 200 TB & \\\colhline
Radioactive source calibration & 100 TB & $<10$ Hz source rate; single
APA readout; 5.4 ms readout \\\colhline
Laser calibration & 200 TB & 10$^6$ total laser pulses; half the
TPC channels illuminated per pulse; lossy
compression (zero-suppression) on all channels\\\colhline
Random triggers & 60 TB & 45 per day\\\colhline
Trigger primitives & $13$ PB & Dominated by $^{39}$Ar (50~kHz per APA face); collection
channels only; 20 bytes per trigger primitive \\\colhline
\end{dunetable}

Self-triggering on \dword{snb} activity is a unique challenge for the
DUNE FD, and an aspect of the design which has never been demonstrated
in a LArTPC. The challenge with \dword{snb} triggering is two-fold. 
First, the activity of the individual \dword{snb} neutrino interactions
is expected to be of relatively low energy ($\SIrange{10}{30}{MeV}$),
often indistinguishable from radiological background activity in the
detector.  Triggering on an ensemble of \bigo{100} events expected on
average in the case of a galactic supernova burst is therefore
advantageous; however, since this ensemble of events is expected to occur sparsely over the
entire detector and over an extended period of \bigo{10}\si{s},
sufficient buffering capability must be designed into the system. 
Furthermore, to assure high efficiency in collecting \dword{snb} interactions
that individually are below individual interaction activity threshold, data from
all channels will be recorded over an extended and contiguous period of
time \bigo{100}\si{s} around every \dword{snb} trigger.

\input{generated/req-longtable-SP-DAQ} 

\subsubsection{Practical Considerations for Design}

The DAQ system is designed as a single, scalable system which can service
all FD modules. It is also designed on the principle that the system should be
able to record and store full detector data with zero dead time; and
that it should be evolutionary, taking advantage of the staged
construction for the DUNE FD, and thus beginning very conservatively
for the first DUNE FD module, and agressively reducing the design
conservatism as further experience is gained with detector
operations. At the same time, it is designed to preserve the possibility of additional capacity
as required. The bulk of processing and buffering of raw detector data is
done underground, in the front-end part of the system (see Figure~\ref{fig:fd-daq:layout}), in order to
minimize data traffic to surface. Power, cooling, and space
constraints in the CUC are limited to 600 kW total and 52 racks for all four FD
modules.

There are three key challenges for the DUNE FD DAQ system: 
\begin{itemize}
\item First, the system must accommodate
a long (``permanent'') commissioning state for the far detector, and
must therefore be a fully ``partitionable'' system. 

Given operational considerations, and in particular based
on the need to minimize \dword{snb} dead time, partitioning the \dword{daq} system allows 
a significant portion of the detector to remain physics-operational
even if a fault interrupts data collection in
some part of the detector. 
This partitionable operation mode also
permits detector development and specialized runs (e.g.,~calibrations)
to run in parallel with normal physics data taking, for small subsets
of the detector.

\item Secondly, the \dword{snb}  physics
requirements necessitate large buffering in the upstream DAQ and low fake
supernova burst trigger rates. 

The implementation of a continuous storage element in the data flow
architecture allows for the formation and capture of delayed, data-driven
trigger decisions with minimal loss of physics information. The
specification for this look-back buffer is set in consultation with
physics groups. It is driven primarily by the need to record up to ten
seconds of unbiased data preceding a \dword{snb} (with the
neutronization time taken as the time of the burst), and it is
specified to be greater than four seconds. This four-second
buffering provision works in tandem with a trigger latency
specification of less than one second. This aspect remains to be validated
with simulation, to ensure that high coverage (greater than 90\%) for galactic \dwords{snb} is achieved by the \dword{snb} trigger.

The \dword{daq} system is also designed so as to be able to %capable of
apply lossless compression to these records, as well as
%additionally 
filter them %in order 
to remove unnecessary data regions
in an intelligent way, i.e.~without compromising physics performance.

A programmable trigger priority scheme ensures %be provided, and
%implemented in a way 
that the readout for the main physics triggers
is never or rarely inhibited so as %, in order 
to enable easy determination of the live-time of
these triggers. % to be easily determined. 
At the same time, generation
of overlapping triggers will be possible, %but 
and ordering and prioritization will %be used to 
prevent data readout duplication. 

\item Finally, the difficult-to-access
location requires that the DAQ operates with high reliability and fully remote operation.

Furthermore, to ensure minimal impact to overall detector live-time, the \dword{daq} system is fully configurable,
controllable, and operable from remote locations, with
authentication implemented to allow exclusive control. It 
furthermore facilitates online monitoring of the detector and of
itself. %the \dword{daq} system in itself.

\end{itemize}

\subsection{Summary of Key Parameters}
\label{sec:sp-daq:parameters}
\fixme{single-phase module}

Table~\ref{tab:sp-daq:parameters} summarizes %all 
the important parameters
driving the \dword{daq} design. These parameters set the scale of
data buffering, processing, and transferring resources which must be
built into the design of each \dword{fd} module. 
\fixme{Table should use standard latex for numbers and units. Anne}

\begin{dunetable}
[Key \dword{daq} Parameters]
{ll}
{tab:sp-daq:parameters}
{Summary of important parameters driving the \dword{daq} design.}
Parameter & Value \\ \toprowrule
TPC Channel Count per Module & 284,000\\ \colhline
TPC Collection Channel Count per Subdetector (APA) & 960\\ \colhline
TPC Induction Channel Count per Subdetector (APA) & 1,600\\ \colhline
PDS Channel Count per Module & TBD\\ \colhline
TPC \dword{adc} Sampling Rate & 2 MHz\\ \colhline
TPC \dword{adc} Dynamic Rate& 12 bits\\ \colhline
Localized Event Record Window & 5.4 ms\\  \colhline
Extended Event Record Window &  100 s\\  \colhline
Full size of TPC Localized Event Record per Module & 6.22 GB \\  \colhline
Full size of TPC Extended Event Record per Module & 115 TB\\  \colhline
\end{dunetable}


\section{Interfaces}
\label{sec:sp-daq:interfaces}

The \dword{daq} system scope begins at the optical fibers streaming raw digital data from the detector active components
(TPC and PDS), and ends at a wide area network (WAN) interface that
distributes the data from on site at \surf to offline centers off
site. The \dword{daq} also provides common computing and network services for
other \dword{dune} systems, although slow control and safety functions
fall outside \dword{daq} scope.

Consequently, the \dword{dune} \dword{fd} \dword{daq} system interfaces with the TPC \dword{ce}, \dword{pds}
readout, computing, \dword{cisc}, and calibration systems of the %\dword{dune}
\dword{fd}, as well as with facilities and underground installation. The
 interface agreements with the \dword{fd} systems 
are summarized in Table~\ref{tab:sp-daq:interfaces}, and described
briefly in the following subsections. Interface agreements with
facilities and underground installation are described in Section~\ref{sec:sp-daq:production}.


\begin{dunetable}
[\dword{daq} System Interface Links]
{p{0.3\textwidth}p{0.3\textwidth}p{0.2\textwidth}}
{tab:sp-daq:interfaces}
{Data Acquisition System Interface Links }
Interfacing System & Description & Reference \\ \toprowrule
TPC CE & & \citedocdb{6742}{v6}\\ \colhline
PDS & &  \citedocdb{6727}{v2} \\ \colhline
Integration Facility & & \citedocdb{7042}{v0} \\
Facilities &&  \citedocdb{6988}{v1} \\ \colhline
CISC & & \citedocdb{6790}{v1} \\ \colhline
Calibration & Constraint on total volume of the calibration data;
trigger and timing distribution from the DAQ & \citedocdb{7069} \\ \colhline
Computing &&  \citedocdb{7123} \\ \colhline
Timing &&  \citedocdb{11224} \\ \colhline
\end{dunetable}

\subsection{TPC Cold Electronics}

The \dword{daq} and TPC \dword{ce} interface is described in
\citedocdb{6742}. The physical interface is at the \dword{cuc}, where optical links from the \dwords{wib} transfer
the raw TPC data to the \dword{daq} \dword{fe} readout (\dword{felix}; see
Section~\ref{??}). This ensures the \dword{daq} is electically decoupled from the detector
cryostat. Ten \SI{10}{Gbps} links are expected per \dword{apa}, and have
been specified as 300m OM4 multi-mode fibers from \dword{sfp}+ at the \dword{wib} to
\dword{minipod} on \dword{felix}. The data format has been specified to use no
compression and custom communication protocol.

\subsection{PDS Readout}

The \dword{daq} and \dword{pds} readout interface is described in
\citedocdb{6727}. It is anticipated to
be of the form of 150  \SI{10}{Gbps} OM4 fibers from one FD module. 
This
is similar to the interface to the TPC \dword{ce}, except the overall
data volume is lower by an order of magnitude. The data format has been specified to use
compression (zero supression) and custom communication protocol.
%\metainfo{Data reception physical and logical, configuration information delivery.}

\subsection{Computing}

The \dword{daq} and computing interface is described in \citedocdb{7123}.
\metainfo{Buffer disk.  Agreement on system administration support and computer
  procurement, ssh gateways, non data networks.  Address reference how
  the data model described above is acceptable.}
 The computing consortium %online computing coordination 
 is responsible for the online areas of WAN connection between \surf and
\fnal, while the \dword{daq} consortium is responsible for disk buffering
to handle any temporary WAN disconnects and the infrastructure needed
for real-time data quality monitoring.  The computing consortium 
is also
responsible for the offline development and operation of the tools for data
transfers to \fnal. The primary
constraint in defining the \dword{daq} and offline computing interface is the
requirement to produce less than \SI{30}{PB/year} %to be transferred 
for transfer to
\fnal. \dword{daq} and %offline 
computing consortia are jointly responsible for data
format definition and data access libraries, as well as real-time data
quality monitoring software. The former is specified in the form of a 
data model documented in \citedocdb{??}.

\subsection{CISC}

\label{sec:sp-daq:interfaces-cisc}
The \dword{daq} and \dword{cisc} interface is described in
\citedocdb{6790}. The \dword{daq} provides a network in the \dword{cuc} for \dword{cisc},  operation information and hardware
monitoring information to \dword{cisc}, and power distribution and
rack status units in \dword{daq} racks. The information from \dword{cisc}
feeds back into the \dword{daq} for run control operations.

\subsection{Calibration}

The \dword{daq} and calibration interface is described in
\citedocdb{7069}. Two calibration systems are envisioned for the
%\dword{dune} 
\dword{fd}: a laser calibration system and a neutron
generator. Calibration pulses can be issued either by the \dword{daq} or by
the calibration systems themselves;  %, and those 
the latter are to be distributed through the
\dword{daq} timing system.

\subsection{Timing Subystem}

%Because 
The timing system of the \dword{dune} \dword{fd} connects with
almost all detector systems and with the calibration system and has a uniform interface to each of
them.
A single interface document,
\citedocdb{11224}, describes all these timing interfaces. 

Accuracy of timestamps delivered to  detector endpoints will be $\pm$\SI{500}{\nano\second} with respect to UTC.  Synchronization between any two endpoints in the detector will be better than \SI{10}{\nano\second} on average.   Between detector modules, synchronization will be better than \SI{25}{\nano\second} on average.  The timing system will also provide a synchronized clock source by which DAQ computer system clocks may be synchronized using standard network time protocols.  System clocks are expected to be synchronized to within a \si{\milli\second} using NTP and \si{\micro\second} using PTP standards. 


\section{Data Acquisition System Design}
\label{sec:fd-daq:design}

This section begins with an overview of the \dword{daq} design followed by %sections giving the design of the \dword{daq} 
brief descriptions of the subsystem design implementation specific. The
implementation details are evolving rapidly; as such, more information
is provided in technical notes as listed in Table~\ref{tab:sp-daq:tech-notes}. 

\begin{dunetable}{p{0.7\textwidth}p{0.2\textwidth}}{tab:sp-daq:tech-notes}{Summary of %relevant and 
detailed \dword{daq} technical notes.}
  Title & Reference \\ \toprowrule
  \dword{dune} \dword{fd} Data Volumes & \citedocdb{9240}\\ \colhline
  The \dword{daq} for the \dword{dune} prototype at CERN &
  \citedocdb{8708}\\ \colhline
  A System for Communication Between \dword{daq} Elements & \citedocdb{10482}\\\colhline
  Data Selection for \dword{dune} Beam and Atmospheric Events & \citedocdb{11215}\\\colhline
  Data orchestrator and event building for \dword{dune} \dword{fd} \dword{daq} & t.b.d. \\\colhline
  \dword{dune} Run Control, Configuration \& Monitoring (CCM) & t.b.d. \\\colhline
  \dword{dune} \dword{daq} Readout & t.b.d. \\\colhline
  DUNE FD Timing and Synchronization System & \citedocdb{11233} \\\colhline
  What are the DUNE FD \dword{daq} Bottlenecks? & \citedocdb{11461} \\\colhline
\end{dunetable}


\subsection{Overview}
\label{sec:fd-daq:design-overview}

The  DAQ system is composed of six distinct subsystems: (1) front-end
readout, (2) data selection, (3) back-end DAQ, (4)
inter-process communication (IPC), (5) control,
configration, and monitoring (CCM), and (6) timing and
synchronization. Each of these subsystems is described in further
detail in the following subsections. The physical extent
of the DAQ subsystems, with the exception of the IPC and CCM,
can be specified in reference to Fig.~\ref{fig:fd-daq:layout}: the
front-end readout and timing distribution live underground in the CUC; data selection occupies
both underground and above-ground spaces; back-end DAQ is above-ground
and includes event building and buffering before distribution of data
to offline; and IPC and CCM extends throughout the entire physical layout of the
system, supported on a private network throughout the DAQ system.. 

The overall system functionality is illustrated conceptually in
Figure~\ref{fig:daq-conceptual-overview}, while Figure~\ref{fig:daq-design}
specifies the implementation. Front-end readout is carried out by custom data receiver and
co-processing FPGA/CPU hardware, all of which is hosted in O(100) servers in the CUC. A
similar number of additional servers is responsible for the execution of
additional software-based low-level processing of trigger primitives
generated in the front-end readout for the purposes of data selection; the collective
low-level information (trigger primitives and trigger candidates
constructed from trigger primitives) is propagated to a central server responsible
for further processing and module-level triggering; the module level
trigger also
interfaces to a second server which is responsible for receiving and
propagating cross-module and external trigger and timing
information. The module level trigger considers trigger candidates and
external trigger inputs in issuing a trigger command to the back-end DAQ
subsystem. The back-end DAQ subsystem 
facilitates event building in O(10) servers and buffering for built
events on non-volatile storage; upon receiving a trigger command, the back-end DAQ queries
data from the front-end readout buffers and builds that into an ``event
record'', which is temporarily stored as (a number of) files. Event records can be optionally processed in a high-level
filter/data reduction stage, which is part of overall data selection, for
further down-selection, prior to becoming custody of the DUNE offline
system. Pervasively, the  \dfirst{daqccm} subsystem provides the central orchestration (Section~\ref{sec:fd-daq:design-run-control}), the \dfirst{ipc} subsystem provides overall communication (Section~\ref{sec:daq:design-ipc}), and the \dfirst{daqtss} provides synchronization (Section~\ref{sec:sp-daq:design-timing}).

\begin{dunefigure}{fig:daq-conceptual-overview}{\dword{daq} Conceptual
   Overview of DAQ System Functionality for a single 10 kton module}
  \includegraphics[width=0.95\textwidth]{daq-toplevel-conceptual.pdf}
\end{dunefigure}

\begin{dunefigure}{fig:daq-design}{\dword{daq} DAQ Design
    Implementation for a single 10 kton module}
  \includegraphics[width=0.8\textwidth]{daq-overview.pdf}
\end{dunefigure}

Key to the implementation of the DAQ design is the requirement that
the system is partitionable. Specifically, the system can operate in
the form of multiple independent DAQ instances, each 
executed across all DAQ subsystems and uniquely mapped among subsystem components. 
More specifically, a given partition may span the entire %up to one
\dword{detmodule} or some subset of it; its extent is configurable at
run start. This ensures continual readout of the
majority of the detector in normal physics data-taking run mode, while
enabling simultaneous calibration or test runs of small portion(s) of the
detector without interruption of normal data-taking. 
Partitioning is further described in Section~\ref{sec:daq:partition-lifetime}.

\subsection{Front-end Readout}
\label{sec:daq:design-readout}
%\fixme{module-generic}

Front-end readout provides the first link in the data flow chain of
the \dword{daq} system and is where raw data from detector electronics
is received by the DAQ.
It implements a receiver, buffer, and a portion of low-level data
selection (trigger primitive generation) as detailed in Figure~\ref{fig:daq:readout}.
It is physically connected to the detector electronics via optical fiber(s) and buffers and serves data to other \dword{daq} subsystems, namely the \dword{daqdsn} and the event builder

\fixme{It would be nice to redraw this using \dword{dune} colors.}

\begin{dunefigure}{fig:daq:readout}{\dword{dune} \dword{daq} front-end
    readout subsystem and its connections.}
  \includegraphics[width=0.8\textwidth]{daq-readout.png}
\end{dunefigure}

\fixme{Would be useful to add a figure showing functional blocks and
  implementation. Georgia}

The readout system comprises many similar \dword{daqrou}, each
connected to a subset of electronics from a detector module and
interfacing with the \dword{daq} switched network. In the case of the
TPC, 75 RU are each responsible for the readout of raw data from two
APAs. In the case of the PDS, 6-8 RU are each responsible for the
readout of raw data from a collection of PDS subdetectors, where each
collection corresponds to an optically isolated region in the
detector. 
Each RU encompasses a commercial off-the-shelf server that hosts a
collection of custom hardware, 
firmware, and software that collectively form four functional blocks:

\begin{enumerate}
\item Data reception, facilitated by a FELIX card in the host server
\item Network based I/O, facilitated by a commercial off-the-shelf network card
\item Data processing, facilitated by FPGA resources on the FELIX
  card and/or on-host CPU resources, or, in the case of the TPC RU only,
  additional FPGA resources in the form of two dedicated co-processing
  boards (interfacing directly with the FELIX card)
\item Temporary data storage, facilitated by host RAM and SSD, or, in
  the case of the TPC RU only, RAM
  and SSD available on the co-processing boards
\end{enumerate}

Each of these blocks is described below.  In addition, and like all other \dword{daq} subsystems, the readout participates in the common software framework for control, configuration, and monitoring as described in Section~\ref{sec:fd-daq:design-run-control}.

\subsubsection{Data reception}

The physical interface between the detector electronics and the \dword{daq} to transmit data consists of 10 Gbps point-to-point serial optical links, running a simple (e.g., 8/10 bit encoded) protocol. 
The number of links per \dword{dune} module varies from approximately 1000 to 2000, depending on the detector technology adopted.

To minimize the space and power consumption footprint of the \dword{daq}, 10-20 links are aggregated into \dword{felix} boards hosted in commercial, off-the-shelf computers.
\dword{felix} is a \dword{fpga}-based PCIe board developed initially for ATLAS and now proposed or already used in several experiments, including \dword{protodune}. 
Existing firmware has been adopted and is being adapted to ensure
decoding and format checking of incoming data and then to marshal the
data to other blocks of the readout subsystem.

\subsubsection{Network based I/O}

The readout subsystem provides access to the \dword{daqdsn} and \dword{daqbes} through a commercial, off-the-shelf switched network as illustrated in Figure~\ref{fig:daq:readout}).
The network communication protocol is as described in Section~\ref{sec:daq:design-ipc}.
The network I/O is handled by the \dwords{daqrou} via software; dedicated hardware or firmware development is not required.

\subsubsection{Data processing}

The data processing functional block resides either on the FELIX board FPGA, or on
an additional, dedicated co-processor providing additional FPGA
processing resources, or both. Data processing can also be carried out in the host
server processor. This functional block is ultimately
responsible for identifying regions of interest in the
detector (in the TPC or PDS) as a function of time.

As a preliminary step, data is pre-processed, i.e., organized in a way
that better suits subsequent data analysis. This implies, e.g.~reorganizing
data into different streams (collection plane vs. induction plane(s),
or re-arranging time and channel order and aggregating samples into frames), applying noise filtering algorithms, and compressing or zero-suppressing data.

The readout system summarizes the identified regions of interest
on a per-channel basis into information packets
called \dwords{trigprimitive}.  These are forwarded to the
\dword{daqdsn} system which makes correlations and ultimately decides whether and which data is to be saved.

This functional block may be implemented onto FPGAs, GPUs, CPUs, or a
combination of these elements. Deciding on the implementation is
premature at this stage; this is one of the main topics to explore and
develop in the readout area. The DAQ design can facilitate either FPGA or
CPU implementation interchangeably, which provides flexibility and
adaptability to whatever the processing needs may ultimately be,
depending, for example, on noise levels in the detector.

\subsubsection{Buffering}

In \dword{dune}, the readout system is in charge of buffering all
detector data until the \dword{daqdss} has formed a trigger decision
(Section~\ref{sec:sp-daq:design-selection-algs}) and until subsequently the
\dword{daqbes} (Section~\ref{sec:fd-daq:design-backend}) has requested and received that selected data. 
In addition, in the case of a \dword{snb} trigger, data received after
the issuing of the trigger must be buffered for much longer to absorb the
strongly punctuated bottlenecks that are expected to form downstream in that case.
%There are two rather different time scales and data throughput metrics which must dictate the technology and the scaling required for the temporary storage providing the buffering.
%As described in Section~\ref{sec:sp-daq:design-selection-algs} these are due to localized and extended activity.

The buffering time required to select data containing localized activity is dominated by processing speed, pipeline depths and network latency. 
Some studies must still be performed but initial estimates indicate that the time buffering time required is should not exceed a maximum of approximately one second. 
The duration of data that must be copied from buffering in order to
capture the localized activity corresponds to 5.4\si{\milli\second}.
As the full stream of data must constantly be buffered, a RAM
technology is selected based on providing sufficient throughput,
endurance, and capacity. 

The extended activity (e.g., due to a potential \dword{snb}) presents a far more challenging set of buffering requirements.  
Low-energy activity that is associated with a \dword{snb} trigger
decision may exist for as much as \snbpretime prior to the issuing of \dword{snb}
trigger (trigger time).
%This below threshold activity may be extracted using more sophisticated and processing intensive algorithms if the data can be recorded. 
A second challenge in recording data containing extended activity is
that all channels must be recorded for 100 seconds around its trigger
time, and requires extracting as much as 115 TB from the TPC readout.
It is not cost effective to design the \dword{daq} to accept this rate
pulse in real time. Thus, additional buffering is provided to catch
the temporary backlog of data that the \dword{snb} trigger will produce.

The technology and scale of this additional buffering must satisfy several requirements. 
It must accept the full data rate of the detector module (as much as \SI{2}{\tera\byte/\second}). 
The data must then reside on nonvolatile media. 
The media must have sufficient capacity and allow for sufficient extraction throughput so that it is vanishing unlikely to be allowed to become too occupied to accept another pulse of data. 
Assuming that, on average, a \dword{snb} trigger condition will be
satisfied once per month, the most optimal technology is %the use of 
solid-state devices, which at the scale required to provide suitable input bandwidth, can provide a capacity to write the data from several extended activity triggers.
Providing only a modest overhead to normal operations, this data can be extracted from such storage from the \dword{daq} in under a day.

For both types of activity, the buffering requirements may be reduced by employing lossless compression to the data prior to it entering the buffer.
A factor of at least two in reduction in buffer input rate is
expected, based on MicroBooNE and protoDUNE experience. 
If expected noise levels are achieved, this compression would provide a factor of four to ten, depending on the detector module technology.
Effort is currently underway to understand the costs and technology involved in exploiting this benefit.

\subsection{Data Selection}
\label{sec:sp-daq:design-selection-algs}

%\fixme{There are many studies which could go into this section. Some of this may very likely be moved into one or more tech notes and referenced.}


%The \dword{daq} must record all non-\dword{snb} unbiased\footnote{In the case of PDS readout, the PDS system only reads out biased
%  information; here it is implied that no additional bias is introduced by the DAQ.} data that span the maximum drift time window for any non-\dword{snb} trigger. 
%This time span is configurable at run start within a range between 20 microseconds and 30
%milliseconds, and it is nominally set to 5.4 ms (driven by the drift
%time). For \dword{snb} triggers, the \dword{daq} must record unbiased
%data spanning up to 100 seconds.
%%is to be recorded. 

%The data-driven trigger decision will be facilitated within the data selection subsystem, which will be an online, primarily software-based system, implemented accross multiple levels of data selection for both TPC and PDS readout. 

%The \dword{daqdsn} subsystem, an online, primarily software-based system implemented accross multiple levels of data selection for both TPC and PDS readout, will facilitate the data-driven trigger decision. 
%Information from the
%lower \dword{daqdsn} levels is aggregated and correlated at the
%\dword{detmodule} level to form a module-wide trigger command, intended to
%prompt module-wide readout.
%Some trigger information will be indirectly propagated between \dwords{detmodule} in order to facilitate a higher-efficiency \dword{snb} triggering than would be possible at the module level alone.
%This multi-level scheme provides necessary flexibility for the
%implementation and scalability of the \dword{daq} system. 


The \dfirst{daqdsn} subsystem is a hierarchical, online, primarily
software-based system. It is responsible for immediate and continuous processing of a substantial fraction of the entire input data stream. 
This includes data from TPC and PDS subdetectors.
From that input, as well as external inputs provided, for example, by
the accelerator or detector calibration systems, the \dword{daqdss} must form a \dword{trigdecision},
which in turn produces a \dword{trigcommand}.
This command summarizes the observed activity that led to the decision
and provides addresses (in space and time) of the data in the \dword{fe} buffers that capture information about the activity.
This command is sent to and then consumed and executed by the \dword{daqbes} as described in Section~\ref{sec:fd-daq:design-backend}. 
It is also propagated to the \dword{etl} and from there it may be
distribute it to other \dwords{detmodule} or other detector systems
(e.g. calibration) for consideration.

The pipelines of processing required for \dword{daqdsn} may execute in
various stages and forms using different firmware and software
implementations. 
Development is actively ongoing to demonstrate
viability and performance of different implementations. In satisfying
the philosophy and strategies of the \dword{daq} design there is 
flexibility in defining whether each element of a pipeline executes on
\dword{fpga}, CPU, GPU, or in principle, some other future hardware
architecture. 

The \dword{daqdss} must select data associated with calibration
signals, as well as beam interactions,
atmospheric neutrinos, rare baryon-number-violating events, and cosmic
ray events that deposit localized visible energy in excess of 100 MeV, with high efficiency ($>$99\%). 
It must also select data associated with potential galactic \dwords{snb}, with galactic coverage\footnote{Galactic coverage is defined as efficiency-weighted probability of galactic \dword{snb}.} of $>$90\% efficiency.
To meet the requirement that the \dword{dune} \dword{fd} maintain a
$<$\SI{30}{\peta\byte/\year} to permanent storage, the \dword{daqdss}
subsystem must make \dword{daqdsn} decisions in a way that allows the \dword{daq} 
system to reduce its input data by almost four orders of magnitude
without jeopardizing the above efficiencies.

The \dword{daqdss} subsystem design follows a hierarchical structure, where low-level
decisions are fed forward into higher-level ones until a  module-level trigger is activated. The hierarchy
is illustrated in Figure~\ref{fig:daq:data-selection-hierarchy}. 
At the lowest level, trigger
primitives are formed on a per-channel basis, and represent, for the
baseline design, a ``hit on
a wire/channel'' activity summary. Trigger primitives are aggregated
into Trigger Candidates, which represent ``clusters of hits''. Trigger
Candidates are subsequently used to inform a module-level trigger decision, which
generates a Trigger Command; this takes the form of either a localized high energy
trigger or an extended SNB trigger, and each prompts readout of an
event record. Post-event-building, further data selection is carried
out in the form of down-selection of event records, through a high
level filter. 

The subsystem structure is illustrated in
Figure~\ref{fig:daq:data-selection}. The structure
has three levels of \dword{daqdsn}: (1) low level trigger, which may consist of
\dword{trigprimitive} generation and subsequent
\dword{trigcandidate} generation; (2) module level trigger; and (3)
\dword{hlt}. Each trigger level is described in subsequent
sections. An additional subsystem component is the external trigger module,
which serves as a common interface for the
module level trigger of each of the FD \dwords{detmodule} and between
the module level trigger and other systems (e.g.,~calibration,
accelerator and timing system) within a single
\dword{detmodule}. After sufficient confirmation of quality the
\dword{etl} also sends \dword{snb} triggers
to global coincidence trigger recipients such as  \dword{snews}.

\fixme{Will remake DS strategy figure. Georgia}

\begin{dunefigure}{fig:daq:data-selection-hierarchy}{Data Selection
    strategy and hierarchy: TPC based.}
  \includegraphics[width=0.6\textwidth]{DS_hierarchy.png}
\end{dunefigure}

\begin{dunefigure}{fig:daq:data-selection}{Block diagram of \dword{dune} \dword{daq}
    \dword{daqdsn} subsystem, illustrating hierarchical structure of
    subsystem design, and subsystem functionality.}
  \includegraphics[width=0.95\textwidth]{DS_summary.png}
\end{dunefigure}


The first stage of DUNE FD operations will have two general classes of trigger
decisions that are categorized in terms of the distribution of activity
in time and space from which they are derived: 
\begin{itemize}
\item Localized activity
    with a 100 MeV deposited energy
    threshold generates localized high energy trigger candidates for
    the module level trigger; any such candidate can be accepted as a localized high energy trigger.
\item    Localized activity with a 10 MeV deposited energy threshold
    generates localized low energy trigger candidates for the module
    level trigger; those are used as input to an extended low energy trigger
    for supernova neutrino bursts. Each trigger type prompts readout
    of the entire detector but over significantly different time
    ranges: localized triggers prompt readout of 5.4 ms event records; extended
    triggers prompt readout 100 s event records. 
\end{itemize}

To facilitate partitioning, the \dword{daqdsn} subsystem will be
able to be instantiated multiple times, and multiple instances will be
able to operate in parallel. Within any
given partition, the \dword{daqdsn} subsystem will also be
informed and aware of current detector configuration and conditions and
apply certain masks and mapping on subdetectors or their fragments in
its decision making. This information is delivered to the
\dword{daqdss} from the \dword{daqccm} system.

Ultimately, each \dword{trigdecision} culminates in a command sent to \dword{daqbes}. 
This command contains all the logical detector addresses and time ranges
required such that an \dword{eb} may properly query the front-end
buffers and finally collect and output the corresponding detector data
and the corresponding trigger data. To avoid duplication of data
records associated with trigger commands that overlap in readout
record ``space'', the \dword{daqdsn} system must also time-order and
prioritize trigger decisions. The details for forming this
command are described next and the operation of the \dword{daqbes} is
described in Section~\ref{sec:fd-daq:design-backend}.

\subsubsection{Low Level Trigger: Trigger Primitive Generation}
\label{sec:sp-daq:design-trigger-primitives}
\fixme{single-phase module}

A \dword{trigprimitive} is defined nominally on a per-channel basis. In the case of
the \single TPC, it is identified as a collection-channel signal rising above some
noise-driven threshold for some minimum period of time (here called a
``hit'').
A \dword{trigprimitive} takes the form of an information packet that 
summarizes the above-threshold waveform information in terms of its
threshold crossing times and statistical measures of its \dword{adc} samples. 
In addition, these packets carry a flag indicating the occurrence of any
failures or other exceptional behavior during \dword{trigprimitive} processing.

\Dwords{trigprimitive} derived from TPC data
are produced in the front-end readout part of the \dword{daq} system,
nominally in FPGA firmware or potentially in CPU or GPU software as described in
Section~\ref{sec:daq:design-readout}.
Any \dwords{trigprimitive} derived from PDS data are produced by the PDS system
for consumption by the \dword{daq} \dword{daqdsn} subsystem.

Algorithms for generating \dwords{trigprimitive} are still under development
\cite{docid-11275}. 
One example algorithm\cite{docid-11236} establishes a waveform baseline
for a given channel, subtracts this baseline from each sample, maintains
a measure of the noise, and searches for the waveform to cross a
threshold defined in terms of the noise level.
This algorithm has been validated using both Monte Carlo simulations and
real data from \dword{protodune}. 
Its performance is summarized in
Section~\ref{sec:sp-daq:design-validation}.

The format and schema of \dwords{trigprimitive} require study and
optimization and this may be tightly coupled with the formation of
trigger candidates discussed next. 
Initial estimates show that 20 bytes provides a generous data
representation of \dword{trigprimitive} information. 
The \dword{trigprimitive} rate will be dominated by the rate of decay of naturally occurring
$^{39}$Ar, which is about \SI{10}{\mega\hertz} per module.
This leads to a detector module aggregate rate of
\SI{200}{\mega\byte/\second}.
The subsequent stage of the \dword{daqdsn} must continuously absorb and process this
rate providing trigger candidates as described next.

%\metainfo{Include plot and discussion of \dword{dune} trigger primitive rate in \dword{protodune}. 
%  The fact that this includes many more cosmics will not matter much as the rate is expected to be dominated by Ar39. 
%  Phil has this already but the LAr purity is not yet high enough to see Ar39 across the whole drift distance.}

\subsubsection{Low Level Trigger: Trigger Candidate Generation}

\Dwords{trigprimitive} from individual, contiguous fragments of the detector
module are consumed in order that cross-channel and -time clustering may be
performed and possibly result in the output of trigger candidates.
Once activity is localized in time and channel (``space'') it is
possible to apply a rough energy-based threshold based on combining the
statistical metrics carried by the input \dwords{trigprimitive}.

A trigger candidate packet carries information about all the trigger
primitives that were used in its formation. 
In particular, it provides a measure of the total activity represented
by these primitives.
This measure will be used downstream to allow the final trigger
decision, as described more in the next section.

Prior to output of, a candidate is subject to a selection criteria.
While the selection applied in the previous stage was driven by a
measure of noise, here it is driven by background activity.  
In particular, candidates which are consistent with activity from the
very high rate, low energy $^{39}$Ar decays will be strongly prescaled. 
The higher energy, lower rate but still numerous candidates consistent
with activity from the $^{42}$Ar decay chain will also be prescaled to
an extent. 
Additional studies are expected but nominally individual candidates, or
groups of candidates nearby in detector space in time, with measures of
energy greater than these two types of decays will be passed with little
or no prescaling.


\subsubsection{Module Level Trigger}

Trigger information is further aggregated as all candidates are consumed
by the module level trigger in order to form the final trigger decision. 
The channel and time extent as well as the energy measure of the
candidates are used at this stage to categorize the activity. 
The category drives the algorithm applied to form a decision.
For example, isolated, low energy candidates arriving in coincidence
over the period of a second across the full \dword{detmodule} may be used toward
satisfying a condition that indicates a potential \dword{snb}. 
Individual high energy candidates, or lower energy candidates
distributed over a localized region of detector space and time would be
applied toward satisfying a high-energy condition. 

When a particular condition in a category is satisfied, the trigger
decision is made and a trigger command is formed. 
This packet includes information of the candidates (and primitives)
that were used to form it. 
The decision also provides direction as to what set of detector components
are to be read out and over what time period.
As described at the start of this section, localized activity will lead
to the readout of the entire detector module for a period equal to \SI{5.4}{ms}.  Extended activity triggers (\dword{snb}) will
direct the readout of much longer period of \SI{100}{s}.

The module level trigger will send its produced trigger commands to the
\dfirst{daqbes} for the detector module.  The
\dword{daqbes} will dispatch the command to a \dword{eb} for
execution as described in Section~\ref{sec:fd-daq:design-backend}.

Trigger commands are also sent to the external trigger logic unit which
forwards them to other detector modules. Likewise, the module level
trigger receives trigger commands from other modules, and considers
this information in making its own trigger decision.
This is particularly needed in order to allow for cross-module
coincidences to be formed and thus produce an overall lower threshold for
capturing potential \dword{snb} occurrences. 
The external trigger logic unit will also forward \dword{snb} trigger
commands, after suitable quality confirmation, to external recipients
such as \dword{snews}.

In addition to accepting cross-module triggers via the external trigger
logic unit, the module level trigger also takes inputs from out-of-band sources such as
needed for beam, calibration or random triggering. 
If meaningful sources of triggering information that is external to
DUNE can be provided promptly enough so that the corresponding data
still resides in the front-end buffers, they may also be considered.


\subsubsection{High Level Filter}
\label{sec:fd-daq:design-data-reduction}

The last processing stage in the \dword{daqdsn} subsystem is the
\dfirst{hlt}, which resides in the back-end part of the \dword{daq} and
is further referenced in Section~\ref{sec:fd-daq:design-data-reduction}.
The \dword{hlt} acts on triggered, read out, and aggregated data,
produced by an \dword{eb}. 
It therefore serves primarily to down-select and thus
limit the total triggered data rate to offline, thereby keeping %. This allows to keep
efficiency high in collecting information on activities of interest
while minimizing selection and content bias, and reducing the output data
rate. It may do so via 
further filtering, lossy data reduction, and/or further event
classification. As it benefits from a longer latency (time between
$\sim$Hz-level built events), it can accommodate higher level of
sophistication in algorithms for \dword{daqdsn} decisions.

More specifically, the \dword{hlt} may further reduce the rate of data saved to final output storage by
applying refined selection criteria which may otherwise be prohibitive
to apply to the pre-trigger data stream.  For example, instrumentally-generated signals (e.g.~correlated noise)
may produce trigger candidates that can not be rejected by the module
level trigger and if left unmitigated may lead to an undesired high
output data rate. 
Post processing the triggered data may allow reducing this unwanted
contamination.
% I (bv) don't agree with this next statement.
Furthermore, it can also reduce the triggered data set by further identifying
and localizing interesting activity. A likely candidate hardware
implementation of this level of \dword{daqdsn} is a GPU-based system
residing on surface at SURF.

To fully understand how much and what type of data reduction may be
beneficial, simulation studies are ongoing \citedocdb{xx} and will
necessarily have to be
validated with initial data analysis after
first DUNE \dword{fd} operation. Planned 
development efforts will also be carried out to determine the scale of
processing required by the \dword{fd}.


\subsection{Back-end System}
\label{sec:fd-daq:design-backend}
\fixme{module-generic}

The \dfirst{daqbes} encompasses the output concept and interfaces to the buffer and trigger concepts shown in Figure~\ref{fig:daq-conceptual-overview}. 
It accepts trigger commands produced by the \dfirst{daqdss} as described in Section~\ref{sec:sp-daq:design-selection-algs}. 
It queries the front-end buffer interfaces and accepts returned data as described in Section~\ref{sec:daq:design-readout}. 
Finally, it records trigger commands and the corresponding data to the output storage buffer, from which the data is transferred to the custody of DUNE offline.

\subsubsection{Dataflow Orchestration}

To minimize data extraction latency, the \dword{daqbes} must not serially execute trigger commands to completion. 
This asynchronous execution governed by the \dword{daqdfo} and operates as illustrated in Figure~\ref{fig:daq:backend} and as discussed here:

\begin{dunefigure}{fig:daq:backend}{Illustration of \dword{dune} \dword{daq} back-end operation.}
  \includegraphics[width=0.8\textwidth]{daq-backend.pdf}
\end{dunefigure}

\begin{itemize}
\item \dword{daqdfo} accepts a time ordered stream of \dwords{trigcommand} and dispatches each for execution possibly by first splitting up each command into one or more contiguous segments.
\item Each segment will then be dispatched to an \dword{eb} process as described in Section~\ref{sec:fd-daq:design-event-builder} for execution.
\item Execution entails interpreting the trigger command segment and querying the appropriate \dword{fe} buffer interfaces to request data from the period of time. 
\item Requests and their replies may be sent synchronously, and replies are expected even if data has already been purged from the \dword{fe} buffer.
\item The data received may then undergo processing and aggregation until finally it is saved to one or more files on the output storage system before custody is transferred offline.
\end{itemize}
%\fixme{Add time order requirement on DS above}
%\fixme{Add return-empty-response requirement to buffer above}



\subsubsection{Event builder}
\label{sec:fd-daq:design-event-builder}
%\fixme{module-generic}
%\metainfo{Explain art\dword{daq}, handling of trigger commands by asynchronous, parallel queries to front end Data Selector (but take care not to duplicate between here and in the overview).}

The \dword{daq} back-end subsystem will provide the instances of the \dfirst{eb}.
As described above, each will request selected data from the appropriate \dfirst{daqfbi} as addressed by a consumed trigger command segment. 
An \dword{eb} will aggregate the selected data and potentially apply processing and reduction (Section~\ref{sec:fd-daq:design-data-reduction}) as well as monitor its quality while in flight.
Finally it will record the resulting data to the output storage system.
The final output files shall use data schema and file formats as described in Section~\ref{sec:fd-daq:design-data-model}.



\subsubsection{Data Quality Monitoring}
\label{sec:fd-daq:design-data-quality}

Section~\ref{sec:daq:design:ccm:monitoring} described a monitoring system for the \dword{daqccm} subsystems. 
Monitoring the quality of the information held in the detector data itself is critical to promptly responding to unexpected conditions and maximizing the quality of acquired data. 
A \dword{daq} \dfirst{dqm} system will be developed to provide functionality for the infrastructure, visualization and algorithms required to continuously process a subset of the detector data so that prompt summaries may be provided for human consumption.
This system will be designed to allow it to evolve as the detector and its data is understood during commissioning and early operation and to cope with any evolution of detector conditions.
Many software modules will be developed offline, so the \dword{dqm} subsystem will facilitate their reuse when applied to samples of detector data.


\subsubsection{Data Model}
\label{sec:fd-daq:design-data-model}
\fixme{module-generic}

\metainfo{Describe the data model. 
  This isn't a strict schema just things like how various parts of the
  detector readout map to files, etc.}

\subsubsection{Output Buffer}

\metainfo{Describe the output buffer system, how it's shared with offline, data hand-off prototocols.  Responsibility scope (eg, who handles transfer to FNAL).}

The output buffer system is a hardware resource provided by \dword{daq} and used offline by \dword{dune}. 
It has two primary purposes. 
First, it decouples producing content in operating \dword{daq} from transferring that content from the far site to archive storage units and offline processing. 
Second, it provides local storage sufficient for uninterrupted \dword{daq} operation in the unlikely event that the connection between the \dword{fd} and the Internet is lost. 
Based on very unusual losses of connectivity at major laboratories as well as \dword{fd} sites of other long-baseline neutrino experiments, the output buffer must provide enough storage capacity to retain one week of output given nominal data production. 
The maximum data production rate for the \dword{fd} is set at \SI{30}{\peta\byte/\year}. 
Thus, the output storage buffer must have a capacity of approximately \SI{0.5}{\peta\byte} to service the entire \dword{fd}.
\fixme{How to reference the 30PB/yr requirement?}


\subsection{Inter-process Communication}
\label{sec:daq:design-ipc}
\fixme{module-generic}

The DUNE FD DAQ is an asynchronous, distributed data processing system. 
As such, it consists of loosely connected elements.
The elements or ``nodes'' are connected through forms of \dfirst{ipc}.
Connections include:

\begin{itemize}
\item CCM elements sending control and configuration messages to all DAQ nodes and receiving from them messages containing monitoring information.  (Section~\ref{sec:fd-daq:design-run-control})
\item Passing trigger information through the Data Selection system (Section~\ref{sec:sp-daq:design-selection-algs})
\item Query based readout of upstream DAQ buffers. (Section~\ref{sec:daq:design-readout})
\item Delivery of trigger commands to the back-end and distributed back-end processing prior to writing final output. (Section~\ref{sec:fd-daq:design-backend})
\end{itemize}

The solutions for each form of IPC are currently being evaluated. 
Certain general requirements apply.
For the most part, they must support transport mechanisms that are \textit{reliable} and \textit{robust}. 
Reliable that messages sent must be received fully unless critical failures of endpoints or the transport link occurs.  Robust means that some classes of failures (temporary network congestion or temporary endpoint failure) that might otherwise interrupt communications may be overcome.
The acceptable duration of failure is chosen based on the particular protocol in use.
Related to reliability, the \dword{ipc}  system used by CCM will provide \dword{daqdispre} functionality as described in Section~\ref{sec:fd-daq:design-run-control}.

The ZeroMQ~\cite{zeromq} smart network socket library is being evaluated for providing a basis for IPC covering CCM, Data Selection and readout queries.  
Section~\ref{sec:fd-daq:validation} contains description of some initial validation of this approach by adding a self-triggering feature to ProtoDUNE-SP which is close to what will be required for the DUNE FD DAQ.  Also described initial prototyping efforts based on ZeroMQ of core elements required for the IPC to support CCM and readout queries.

The artDAQ system~\cite{artdaq} utilizes IPC for connecting its various elements.  It is full featured and well tested in production settings including in ProtoDUNE-SP.  It is a natural choice for providing a basis for the DAQ back-end.  It will also be considered for providing a basis for the Data Selection system.  Upstream DAQ readout queries may also be implemented in terms of artDAQ elements.

Additional R\&D and validation are needed to determine an optimal solution for each IPC domain.  This optimization process will take into account requirements unique to DUNE FD (minimal downtime, data rates, variation in detector modules, long term use, available expertise, flexibility for future developmental improvements, etc).

\subsection{Control, Configuration, and Monitoring}
\label{sec:fd-daq:design-run-control}
% \fixme{module-generic}

% \metainfo{Describe run control and \dword{daq} operation monitoring. 
%   How it makes use of the Message Passing System. 
%   What is an ``epoch''.
%   How Epoch Change Requests lead to zero downtime reconfiguration. 
%   Public key based ``iron'' authentication between run control processes and the controlled processes. 
%   Describe how RC will configure partitioning, initiate reconfiguration, handle ``run'' changes, node discovery, configuration, logging, startup, shutdown and failures are handled. Describe how RC will support detector electronics configuration.}

The \dfirst{daqccm} subsystem, illustrated in Figure~\ref{fig:daq-ccm-subsys}, encompasses, as its name suggests, the software needed to control, configure, and monitor the rest of the \dword{daq}, as well as itself. 
It provides a center for the highly distributed \dword{daq} components, allowing them to be treated and managed as a single, coherent system. 
Figure~\ref{fig:daq-conceptual-overview} shows the central role of the \dword{daqccm} within the complete \dword{daq} system.
The following sections describe each of the three \dword{daqccm} subsystems. 

\begin{dunefigure}{fig:daq-ccm-subsys}{Main interaction among the three \dword{daqccm} subsystems.}
  \includegraphics[width=0.8\textwidth]{daq-ccm-subsys.pdf}
\end{dunefigure}

\subsubsection{Control}
\label{sec:daq:design:ccm:control}


The \dword{daq} control subsystem actively manages \dword{daq} software process lifetimes, asserts access control policies, executes commands, initiates configuration changes, detects and handles exceptions, and provides an interface for human operators.

% \fixme{I (bv) redrew this from Giovanna's to get vector PDF and match \dword{dune} color palette.  I made two content changes: 1: move UI out of partition as it can't both initiate a partition and be inside it and likely will allow for creation/viewing of more than just one partition.  2: I had to guess on how garbage collection might go and so addded a line from RM to PM. }

\begin{dunefigure}{fig:daq-ccm-control}{Roles and services that compose the \dword{daq} control subsystem.}
  \includegraphics[width=0.8\textwidth]{daq-ccm-control.pdf}
\end{dunefigure}

The control subsystem comprises a number of functional blocks of either global or partition scope as illustrated in Figure~\ref{fig:daq-ccm-control}. 
In the figure, ``partition'' refers to a logical segmentation of the \dword{daq} where each segment operates to some extent independently from others. 
The segmentation applies to the portion of detector electronics that provides input to the partition's \dword{daqdsn} and readout. 
The largest partition %would be 
is that of one \dword{detmodule}. 
The functional blocks in the figure represent one or more semi-autonomous agents, each with defined roles, capabilities, and access. 
Although drawn as single blocks, they typically are implemented as multiple peer agents to assure redundancy and fail-over. 
The blocks at partition scope are first described.

\begin{itemize}
\item Partition naming service provides \dword{daqdispre} for the components of a \dword{daqpart}. 
  That is, it allows a component to be made aware  of the creation and continued operation of other components (discovery) or that other components have recently become unresponsive (presence). 

\item Run control provides a central director;  its creation is the first step in initiating a \dword{daqpart}. 
  The \dword{rc} accepts, interprets, and validates input commands that may come either from a human via a user interface, or from other blocks. 
  The commands describe a desired state of the \dword{daqpart}. 
  \dword{rc} may query other blocks to validate commands and then execute the commands by allocating processes through process management. 
  Once successfully allocated, their lifetimes are managed by \dword{rc}. 
  Throughout its lifetime, \dword{rc} may reconfigure an existing process, destroy it, or allocate additional processes. 
  \dword{rc} may query the partition-naming service in order to resolve resource identifiers in commands into their corresponding network endpoint addresses.  

\item Supervisor provides a locus of expert system automation. 
  This block is initiated along with its \dword{rc} peer and augments human commands with automated ones. 
  For example, as certain ``common exceptions'' are encountered and understood so that a means to correct them can be developed, the supervisor can be extended to automatically issue the corrective actions that must otherwise be manually performed by a human operator.

\end{itemize}

Global scope controls \dword{daq} components for all \dwords{daqpart} across all \dwords{detmodule}\footnote{\dword{daq} instances at locations other than the \dword{fd} cavern are expected to operate in a wholly distinct manner.}.  It consists of the following blocks:


\begin{itemize}
\item Global naming service aggregates the \dword{daqdispre} information across partitions.  Like the partition naming service, this may be implemented as a centralized (but redundant) explicit service or may be provided by extending the IPC \dword{daqdispre} mechanism to the entire FD \dword{daq} network.

\item Process management allocates and may reclaim sets of processes on behalf of a requesting component (specifically \dword{rc} and resource management). 
  An allocation request includes a complete description of the processes and their desired initial configuration. 
  A successful allocation occurs only after this role successfully initiates all requested processes and confirms their presence. 
  Process management only allocates processes if their requester has appropriate access privileges as determined by access management and if resource management determines sufficient resources exist. 
  Process management may support pre-allocation if sufficient access and resources are confirmed and reserved; if so, a token (aka ``cookie'') is returned to the requester. 
  This cookie may be subsequently be presented to complete the allocation and claim the processes. 
  After a configured timeout, the cookie may be invalidated.\footnote{This will be required if the race condition between multiple UIs and RCs is a problem.}  
  
\item Resource management determines whether any process allocation can proceed and enacts a process garbage collection mechanism. 
  An allocation is successful if sufficient process resources are available and if it is consistent with a set of configured constraints maintained by this component. 
  Resource management monitors all allocated processes as well as the process initiating a request in order to perform ``garbage collection''. 
  This is performed in the event that the allocated processes outlive the requesting processes.
  Resource management  notifies process management of any remaining processes from an allocation when it detects that the requester is no longer present.
  Resource management also supports pre-allocation validation queries. 
  A validation response merely indicates current state and it represents no guarantee that the allocation will subsequently succeed.

  
\item Access management is responsible for providing authentication and authorization for all \dword{daq} functions that require access control. 
  This block may be implemented as an explicit centralized (but redundant) service or through a distributed IPC mechanism or as some mixture of the two designs. 

\end{itemize}

The last block in Figure~\ref{fig:daq-ccm-control} represents applications that provide user interfaces (UI) to the control subsystem.
At least one UI will be developed to allow a trained operator to construct and issue commands required to initiate, configure, potentially reconfigure, and finally terminate \dwords{daqpart}. 
The UI may validate user commands before issuing them to an \dword{rc} by directly querying the process manager block. 
If sufficient resources are unavailable or if the user lacks appropriate access privileges, the UI will present a descriptive error message or otherwise disable corresponding functionality.  
If the user permission is valid the UI sends the use-initiated command to the \dword{rc} for execution.
Note that subsequent commands from the \dword{rc} to other blocks are also subject to access management.
Additional UI elements will be developed as described in sections~\ref{sec:daq:design:ccm:configuration} and~\ref{sec:daq:design:ccm:monitoring}.


\subsubsection{Configuration}
\label{sec:daq:design:ccm:configuration}

The \dword{daq} configuration subsystem provides persistent data storage for all historic, current, and future configuration information applicable to the \dword{daq}.
It provides a singular point (via high-availability, redundant services) for the allocation of unique and monotonically increasing \dwords{daqrunnum}.
The configuration data stores operate in an insert-only mode with no update nor deletion of records so as to keep a complete record of configuration actions.
The configuration subsystem supports the following types of information:

\begin{itemize}

\item Partition structure contains descriptions of the multiplicity and connectivity of \dword{daq} components for any partition. 
  Structure and connectivity is expressed in an abstract manner with logical addressing and not through concrete addressing (eg, host computer network address and port numbers). 
  This allows for identical structure to be reapplied to various collections of specific hardware.\footnote{It is the discovery and presence from naming services as described in Section~\ref{sec:daq:design:ccm:control} that allow mapping from abstract to concrete addressing.}

\item Component parameters comprise configuration information associated with any given \dword{daq} component.  This data is structured following a schema defined by the associated component and this schema is versioned to allow for schema evolution.

\item Run number provides a monotonically increasing sequence of \dwords{daqrunnum} that are allocated upon request to assure that each is unique.

\item Partition instances associate a \dword{daqrunnum} and the set of component parameters that were used to initiate a \dword{daqpart} or which are used to reconfigure an existing \dword{daqpart}.  

\item Constraints define rules that must be held true by resource management servicing requests for process allocations.  This information store also includes which constraints were used by resource management over time.
\end{itemize}


Access to configuration information is via a service that hides the choice of storage technology from any client queries.
This interface will also be used by configuration editors utilized by human operators as well as any  generators employed by expert systems.

\subsubsection{Monitoring}
\label{sec:daq:design:ccm:monitoring}

The \dword{daq} monitoring subsystem will help both humans and expert systems in detecting, diagnosing, and correcting anomalous activity, observing intended operation, and providing a historical record.
This subsystem will accept required information produced by any \dword{daq} component (here called status).

The precise implementation of the production, acceptance, store, post-processing, querying, and visualization of monitored status requires additional work. 
However, a publish-subscribe (PUB/SUB) network communication pattern is expected to be adopted for transport of monitoring messages. 
This will decouple production and consumption and facilitate development of a variety of status viewers, expert systems, debugging tools, etc. 
The types of messages include but are not limited to the following:

\begin{itemize}
\item Common to all will be a ``header'' holding a message type indicator, a sender address and the associated detector data time and the recent host computer time.
 
\item Logging messages add an importance label (e.g., debug, info, warning, error) and a succinct, human-readable information string providing an explanation of what occurred.
  
\item Metrics will provide structured data carrying specific information about predefined aspects of the sender. 
  This is similar to logging, but the messages support automated consumption and reaction by expert systems.  

\item Quality messages summarize information derived from the detector data (e.g., from waveforms) or its metadata (e.g., timestamps, error codes) while that data is ``in flight'' through the \dword{daq}.

\end{itemize}

In general, the DAQ will retain all status records at least long enough to allow for any offline data quality validation procedures to be performed. 
However, some status feeds may be processed prior to storage if their raw form requires prohibitive amount of storage. 
In particular, the quality stream data rate may be too substantial for long term storage. 
Such streams will be summarized into histograms or other statistical representations prior to storing for longer term use.

In addition to this \dword{daq} \dword{daqccm} monitoring subsystem, a separate system must be used to monitor in depth the quality of the detector data content itself. 
See Section~\ref{sec:fd-daq:design-data-quality} for the description of this data quality monitoring system.

\subsubsection{Partition Lifetime}
\label{sec:daq:partition-lifetime}

The partition lifetime is described here in a somewhat linear narrative, it should be noted that the components will be constructed through some suitable protocol that need not progress in the same linear order. 
In particular, the operation of the partition components shall be robust to the order in which peers are discovered.

After a process is executed via the allocation mechanism described in Section~\ref{sec:daq:design:ccm:control}, it will apply its initial configuration. 
This information includes any personal identifiers the component will assert as part of \dword{daqdispre} as well as any identifiers required to find any other peer components which are needed for its own operation. 
In particular, a component is provided the identity of the partition's \dword{rc} instance so that it may receive control directives.

It is through a control directive enacted by each individual component that the overall partition structure and connectivity emerges.  
These directives must be issued prior to some activation criteria in order to enable the \textit{zero-downtime reconfiguration} feature as described next.
The control directive contains a \dfirst{ccc}.
A \dword{ccc} provides, at a minimum, the following pieces of information:

\begin{itemize}
\item Run number is as described in Section~\ref{sec:daq:design:ccm:configuration}. 
  Here, it identifies a desired and collective partition state which will be constructed once all \dwords{ccc} are enacted.
\item Activation time stamp (ACT) states the \textit{data time} (see Section~\ref{sec:daq:design-ipc}) at which the \dword{ccc} shall take effect. 
\item Configuration payload provides the component-specific configuration to be enacted and may include   actions must be performed prior to the \dword{act} in order to assure a zero-downtime transition.
\end{itemize}

When a \dword{ccc} is received by a component, that component initiates any new connections with peers and performs any other pre-\dword{act} actions as directed by the \dword{ccc}. 
The component then begins (or continues) to monitor the \textit{data time} of received messages on its new (or previous) input sockets. 
If the component was operating as part of the prior partition it will continue to service its previous input and produce output to any previously connecting consumers.
Data is not yet sent to any new connections.
Upon receiving input with a data time after the \dword{act} it will apply the new configuration specified in the \dword{ccc}. 
In this reconfiguration process, any pre-\dword{act} data that may still be buffered by the component shall be flushed to its (previously connected) output sockets. 
Any input or output connections no longer applicable to the new, post-\dword{act} partition definition shall be dropped. 
Finally, the component shall renew operations, beginning with the held data which had satisfied the \dword{act} criteria and which initiated the reconfiguration processes.

For this mechanism to truly provide zero-downtime, the partition components must receive reconfiguration messages from the \dword{ccc} sufficiently in advance of input data passing the \dword{act}.
This means the human-UI-\dword{rc} chain must select an \dword{act} knowing the most recent \textit{data time} as well as some lead time to apply.  
For any given reconfiguration an optimal lead time involves many interdependent factors but may be estimated by considering a maximum calculated over all components involved in the reconfiguration of the time difference between their required reconfiguration time and the latency for data to arrive at the component from the time of sampling. 
In a real system this maximum has some distribution. 
In practice it is expected that the lead time must be chosen in some %empiric 
manner or simply ``long enough''.
Even a generous choice is likely to satisfy human human impatience especially given the alternative is accepting data loss.

Although the lead time may need to be many seconds, it is important to note that the minimum time between subsequent \dword{act}s is essentially zero. 
Multiple sets of \dwords{ccc} may be issued over some short time span and queued by components. 
In principle, this allows zero-downtime sequencing of runs of arbitrarily short duration.
Practically however, this may be limited due to performance issues. 
For example, if a new set of \dwords{ccc} requires many duplicate readers of data streams this may cause bandwidth limits to be reached. 
To the extent this fast run sequencing is needed, these potential limitations require additional study.

Finally, after cycling through one or more run numbers, the partition may be terminated. 
A final round of \dwords{ccc} is issued by the partition's \dword{rc}. 
Each \dword{ccc} directs the termination procedure of its component. 
This procedure starts just as any zero-downtime reconfiguration. 
The \dword{ccc} instructs the component to continue processing until receiving input data which satisfies the \dword{act} criteria at which time any remaining buffered data is flushed to the component output connections. 
Then, unlike zero-downtime reconfiguration, the component simply destroys all connections and exits. 
The \dword{rc} notifies the UI and process manager of the destruction of the partition (saving, for the moment, the destruction of the \dword{rc} itself). 
The process manager notifies the resource manager that the resources have been released. 
The \dword{rc} then terminates itself, and the partition is no more.
The resource manager confirms partition processes have terminated through \dword{daqdispre}. 
In the odd case that the \dword{rc} aborts without cleanly terminating the partition, its absence must be detected, and the remaining partition processes are reaped in the garbage collection mechanism described in Section~\ref{sec:daq:design:ccm:control}.

\subsubsection{Self-healing}
\label{sec:daq:self-healing}

The above zero-downtime reconfiguration mechanism is intentional and typically driven by human action or automated run sequencing algorithms. 
Similarly, the partition will be \textit{self-healing} in the face of unexpected failures that render peers unresponsive or when unexpected information content is received by partition components.  
Extending the metaphor, self-healing involves these phases: \textit{detection} of an injury to the partition, \textit{diagnosis} of the scope of the injury, and \textit{intervention} that executes an action on the partition.

Detection is performed by the \dword{daq} control subsystem supervisor functional block using at least one of two methods.
First, if a component in the partition becomes unresponsive (i.e., it crashes or hangs) the supervisor  receives notification through \dword{daq} \dword{daqdispre}. 
Second, if a component directly detects injury, such as may be the case when it receives  data outside of expected norms, it reports this fact through \dword{ipc} to the supervisor.

When injury is detected the supervisor  diagnoses this using heuristics and other methods that are expected to evolve as failure modes are discovered or removed. 
It is thus crucial that the supervisor is designed and developed in a way that facilitates this ongoing evolution.  

Finally, the supervisor responds to the diagnosis with some intervention. 
Response  in all cases includes notifying the monitoring subsystem. 
Any additional response involves sending commands to the \dword{rc} to initiate a reconfiguration,  which may be to terminate the partition.
When initiating a reconfiguration, as with any reconfiguration, the command to the \dword{rc} must include information required by the \dword{rc} to issue \dword{ccc} messages. 
If the partition remains it is reconfigured and thus begins a new run number as described in Section~\ref{sec:daq:partition-lifetime}.




\subsection{Timing Distribution and Synchronization}
\label{sec:sp-daq:design-timing}
%\fixme{single-phase module}
%\fixme{Is it indeed still single-phase specific?}
%\metainfo{Hardware, consumers, links.}

All components of the \dword{fd} use clocks derived from a single \dfirst{gps} disciplined source, and all module components are synchronized to a \SI{62.5}{MHz} clock. 
To make full use of the information from the \dword{pds}, the common clock must be aligned within a single detector %unit 
module with an accuracy of \bigo{\SI{1}{\nano\second}}. 
For a common trigger for a \dword{snb} between modules, the timing must have an accuracy of \bigo{\SI{1}{\milli\second}}.
However, a still tighter constraint is the need to calibrate the common clock to universal time derived from \dword{gps} so the \dword{daqdsn} algorithm can be adjusted inside an accelerator spill, which again requires an absolute accuracy of \bigo{\SI{1}{\micro\second}}.

The \dword{dune} \dword{fd} uses a version of the \dword{protodune} timing
system, where a design principle is to transmit synchronization messages over
a serial data stream with the clock embedded in the data. The format
is described in \citedocdb{1651}. The timing system design is
described in detail in \citedocdb{11233}.

Central to the timing system are four types of signals:
\begin{itemize}
\item a \SI{10}{\mega\hertz} reference used to discipline a stable master clock,
\item a \dfirst{pps} from the GPS,
\item a \dword{ntp} signal providing an absolute time for each \dword{pps}, and
\item an \dfirst{irig} time code signal
  used to set the timing system 64-bit time stamp.
\end{itemize}
%The timing system associates the \dword{gps} time to its master clock by using the latter to stamp the arrival time of the \dword{pps} of the former. To provide an absolute time to the nearest second the information from \dword{ntp} is used.
%\fixme{This may no longer be correct.  Will NTP be used and the host computer clock consulted for the absolute time?  Will PTP used in its stead?  How does IRIG fit in?  Finally, this sentence, however it needs to be written, is redundant with the itemized list.}

The timing system synchronization codes are distributed to the \dword{daq} readout components in the \dfirst{cuc} and the readout components on the cryostat via single mode fibers and passive splitters/combiners.
All custom electronic components of the timing system are contained in two \dword{utca} shelves; at any time, one is active while the other serves as a hot spare.
The \SI{10}{MHz} reference clock and the \dword{pps} signal are received through a single-width advanced mezzanine card (\dword{amc}) at the center of the \dword{utca} shelf.
This master timing \dword{amc} is a custom board and produces the timing system signals, encoding them onto a serial data stream.
This serial data stream is distributed over a backplane to a number of fanout \dwords{amc}.
The fanout \dword{amc} is an off-the-self board with two custom \dwords{fmc}.
Each \dword{fmc} has four \dword{sfp} cages where fibers connect the timing system to each detector component (e.g., \dword{apa}) or where direct attach cables connect to other systems in the \dword{cuc}.

To provide redundancy, two independent GPS systems are used,
one with an antenna at the surface of the Ross shaft, and the other
with an antenna at the surface of the Yates shaft. Signals from either
GPS are fed through optical single mode fibers to the \dword{cuc}, where
either GPS signal can act as a hot spare while the other is active. 
Differential delays between these two paths are resolved by a second pair of fibers, one running back from the timing system to each antenna.


\section{Design Validation and Development Plans}
\label{sec:fd-daq:validation}

The following strategy will be followed in order to validate and
develop the \dword{dune} \dword{fd} \dword{daq} design:
\begin{itemize}
\item Use of \dword{protodune} as a design demonstration and
  development platform. 
\item Use of vertical slice teststands for further development and testing of
  individual \dword{daq} subsystems and for key aspects of the
  overall \dword{daq}
\item Use of horizontal slice tests to demonstrate scaling the design
  where the multiplicity of components in subsystem layers is important.
\item Use of \dword{fd} MonteCarlo simulations and emulations in order
  to augment actual hardware demonstrations at \dword{protodune} and teststands.
\item Benefit from developments and measurements from other ongoing
  LArTPC experiments, including MicroBooNE, SBND, and ICARUS.
\end{itemize}

This strategy reflects the current \dword{daq} project schedule, which
comprises several phases, including an intense development phase
running through 2020 that culminates in an engineering design
review (EDR) in Q1 of 2021. At this milestone, the system design will be
finalized and shown to be capable of meeting the requirements of the
final \dword{daq} system. After the development phase, a
pre-production phase will begin and will end with a production readiness
review (PRR). By then, final designs of all components
will be complete.

The following subsections summarize past, ongoing, and planned
development and validation studies and identify how anticipated outcomes
will be used to finalize the \dword{daq} design.

\subsection{Design Validation and Development at ProtoDUNE and Other
  LArTPC Experiments}

\label{sec:fd-daq:protodune}
\metainfo{Here we describe protodune, write what similarities and
  differences there are between \dword{protodune} and \dword{dune}
  \dword{daq} designs. Mention other related efforts at other experiments.}

The \dword{fd} \dword{daq} consortium constructed and operated the \dword{daq} system for
\dword{protodune}, which included %facilitated two incarnations of 
two \dword{daq} readout 
architectures, one based on \dword{felix}, develped by ATLAS \cite{xx}, and the other on \dword{rce}, developed at
SLAC \cite{xx}. \dword{daq} design and construction for
\dword{protodune} began in Q3 of 2016, and the system became operational at the start of the beam data run %when \dword{protodune} beam running began 
in Q4 of 2018. The detector is
continuing to run as of the writing of this document, recording cosmic
ray activity, and %facilitating 
providing further input for \dword{daq} development toward
\dword{dune}. 

Figure~\ref{fig:daq-protodune} depicts the \dword{protodune} \dword{daq} system.  The \dword{daq} is split %among %two readout architecture implementations, one involving 
between the  \dword{felix}  and \dword{rce} implementations. The two architectures share the same back-end and
timing and trigger systems. 
Neither of these tested % readout 
architectures %( \dword{felix}  or RCE) 
exclusively represents the
baseline design for the DUNE \dword{fd}. Instead, each %readout architecture
qualitatively maps into one of
two data processing approaches: one in which the data is processed
exclusively in
custom-designed \dword{fpga}, and the other  in which the data is processed primarily in commodity
CPUs. The baseline system for a %DUNE FD 
\dword{detmodule} instead
merges elements of the two approaches. Specifically, it uses  \dword{felix}  as
the hardware platform for data receiving and handling, and an
\dword{fpga}-based co-processor (analogous to the \dword{rce} platform) that
interfaces with  \dword{felix}  to provide additional, dedicated data processing
resources. In that sense,\dword{protodune} has provided demonstration of the
 \dword{felix}  platform as \dword{fe} readout data receiver, and
demonstration of \dword{fpga}-based data reduction for TPC.

\fixme{Placeholder. Should show felix and rce architectures}
\begin{dunefigure}{fig:daq-protodune}{The \dword{protodune} \dword{daq} system.}
%  \includegraphics[width=0.8\textwidth]{daq-protodune.pdf}
\end{dunefigure}

Besides overall readout architecture,
the \dword{protodune} and
\dword{dune} \dwords{daq} exhibit two key differences. 
First, the \dword{protodune} \dword{daq} is externally
triggered (and at a trigger rate 
over an order of
magnitude higher than that anticipated for \dword{dune}). Because of
this, the
\dword{protodune} \dword{daq} 
does not facilitate online data
processing from the TPC or \dword{pd} systems for self-triggering. 
Second, the \dword{protodune} 
system sits at the surface with a much higher
data occupancy 
due to cosmic ray activity.
Overcoming the first key difference to demonstrate \dword{daqdsn} capability for the \dword{fd} \dword{daq} design is a main component of future \dword{daq} development plans, described in Section~\ref{xx}.

Continuous self-tirggering of the detector is also new with respect to
other ongoing or planned near-term LArTPC experiments, including
MicroBooNE, SBND, and ICARUS. Both MicroBooNE and ICARUS have demonstrated
self-triggering in coincidence with external gates, which effectively
limits both data and trigger rates, and is not a viable solution for
DUNE's off-beam physics program, as it would effectively limit
exposure and therefore physics sensitivity. On the other hand, ICARUS
has ...
MicroBooNE has demonstrated successful continuous readout of a LArTPC,
via use of dynamic and fixed-baseline zero-supression implemented in
firmware for both TPC and PDS readout. SBND, which utilizes the same readout
system as MicroBooNE, will investigate trigger primitive generation
and self-triggering based on TPC information in the timescale of 2021-2023.

\fixme{Add references.}

\subsection{ProtoDUNE Outcomes}

Despite its %being a 
variant design %and 
with more limited scope, the %already
successful operation of the \dword{protodune} \dword{daq} has provided several key
demonstrations for \dword{dune} \dword{daq}, in particular data flow
architecture, run configuration and control, and back-end
functionality. % In the following subsection we discuss important lessons learned.

More specifically, \dword{protodune} has demonstrated 
\begin{itemize}
\item Front-end Readout: successful
front-end readout hardware and data flow functionality for the readout
of two out of the six APAs employed in protoDUNE. This was achieved
with two TPC RU's, without co-processor boards, and only one APA read
out per FELIX board. The \dword{dune} DAQ design will ultimately
accommodate readout of two APAs per FELIX board. In addition to data
flow functionality, ProtoDUNE Front-end readout also demonstrates interface to
front-end electronics, and scalability to DUNE. It also supports
host server requirements and specifications. Finally, it serves as
platform for further development involving co-processor implementation
and data selection aspects.
\item Back-end DAQ: successful back-end DAQ implementation, including event builder
  farm, CCM machines, and disk buffering. This has allowed the
  development and exercising of system partitioning, and provides a
  basis for scalability to DUNE. The protoDUNE back-end also serves as
  a platform for further system development, in particular in the
  areas of CCM, IPC and data flow (orchestrator).
\item Data Selection/Timing: successful operation of the timing
  distribution system, and external trigger distribution to the
  front-end readout. Although protoDUNE was externally triggered, the
  system serves
\end{itemize}

Besides demonstrating end-to-end data flow, an important outcome of
protodune daq has been the delineation of
interfaces, i.e.~understanding the exact \dword{daq} scope and the interfaces to TPC, \dword{pds}, and offline. The use of commercial off-the-shelf solutions
where possible, and leverage of professional support from CERN IT 
substantially expedited the development and success of the project, as
did the strong on-site presence of experts from within the consortium during early installation and
commissioning. 

Outcomes specific to \dword{protodune} subsystems are discussed in
greater detail in \[talk from review\].

\subsection{Ongoing Developments}
\label{sec:sp-daq:design-validation}

%\fixme{this section and onward still needs work}

Subsystem development is ongoing at ProtoDUNE at the time of the
writing of this document. A detailed schedule for 2019 is available
in \[referencedocdb\]. Major development plan milestones are:
\begin{itemize}
\item optimization and tuning of the Front-end readout
\item optimization and tuning of the artdaq based dataflow software
\item enhancement of monitoring and troubleshooting capabilities
\item introduction of CPU-based hit finding (necessary for PDS readout)
\item introduction of FPGA-based hit finding (for TPC readout)
\item implementation of online software data selection beyond trigger
primitive stage (introduction of trigger candidate generation, and
trigger command generation), and tests on well identified interaction
topologies (e.g. long horizontal tracks, or Michel electrons from muon decay)
\item integration of online triger command and modified data flow to event
builder to facilitate self-triggering of detector
\item implementation of extended fpga based front-end functionality
(e.g. compression)
\item prototyping of fake SNB data flow in front-end and back-end
\end{itemize}

Below, we focus on ongoing developments related to
data selection, which is a key challenge for DUNE and new with respect
to ProtoDUNE (and other existing or planned LArTPC detectors), as well
as IPC and CCM.

During early stages of design, significant effort has been dedicated on trigger primitive generation
through MonteCarlo simulations. Specifically, charge collection efficiency and fake rates
due to noise and radiologicals have been studied as a function of
hitthreshold with MonteCarlo, demonstrating that requirements can be
met, given sufficiently low electronics noise levels and radiological
signal \[ref\]. Ongoing efforts within DUNE's Radiologicals Task force
aim to validate or provide more accurate background predictions, upon
which this performance will be validated. In addition, offline emulations
of CPU trigger primitive generation on CPU (4 cores) have been carried
out, demonstrating the ability of software algorithms in CPU to keep
up with expected raw data rates, as shown in Figure~\ref{xx}. 
Following the commissioning of ProtoDUNE, full-stream, single-APA,
online trigger primitive generation on CPU (10 cores) was successfully
demonstrated at ProtoDUNE. Trigger primitive rates were measured at
ProtoDUNE in situ. Effort on understanding and removing contribution
from cosmics/cosmogenics and (known) noisy channels is ongoing. 

Trigger candidate generation, building on trigger primitives and defined
as two consecutive trigger primitives in both channel and time space,
with a minimum hit threshold, have also been studied with MonteCarlo
simulations. Trigger candidates with sufficient energy can be accepted
to generate corresponding Trigger Commands for localized high energy
activity, such as for beam, atmospheric neutrinos, baryon number
violating signatures, and cosmics. Simulation studies demonstrate that
this scheme meets efficiency requirements for localized high energy
triggers, as shown in Figure~\ref{xx}. Specifically,
simulations demonstrate that $>99$\% efficiency is achievable for
$>100$ MeV visible energy, and that the effective threshold for
localized triggers for the system is at $\sim$10 MeV. 

Low-energy trigger candidates furthermore can serve as input to the
SNB trigger. Simulation demonstrates that the trigger candidate
efficiency for any individual SN neutrino interaction is on the order
of 20-30\%. Simulations have further demonstrated that a
multiplicity-based SNB trigger decision which integrates low-energy
trigger candidates over an up to 10 seconds
integration window yields high ($>90$\%) galactic coverage while
keeping fake SNB trigger rates to one per month, per system
requirements. An energy-weighted multiplicity count scheme could be
applied to further increase efficiency and minimize background.
The dominant contributor to fake SNB triggers is
radiological backgrounds from neutrons, followed by Radon. It is
crucial to continue working closely with the Radiological Task force
to validate radiological simulation assumptions.

Given that simulation studies support requirements and rate
assumptions, the protoDUNE demonstration of ability to keep up with
rates from 1/25$^{th}$ the size of a single DUNE FD SP module, for trigger
rates up to 40 Hz and 3 ms readout window 
allows confident scaling of the protoDUNE back-end DAQ subsystem to
that of DUNE.

\fixme{Add HLT simulations}

% \subsubsubsection{Ongoing  \dword{felix}  Throughput Demonstration at \dword{protodune}}
% \label{sec:sp-daq:validation-pdune-felix}
% %\fixme{single-phase module}

% %\metainfo{Describe how the  \dword{felix}  \dword{daq} at \dword{protodune} demonstrates a
%  %  \dword{felix} +CPU approach. 
%  % Describe the elements that are same or similar (full-rate to host
%  % RAM buffer) and different (higher-rate but external trigger).}

% The  \dword{felix}  \dword{daq} at \dword{protodune} partly demonstrates the front-end readout scheme for
% the PDS system, as well as the  \dword{felix} +CPU readout approach for the
% TPC. The elements of the \dword{protodune}  \dword{felix}  \dword{daq}, which are the
% same in \dword{dune}, have already demonstrated the
% reception of raw data at full rate from a single \dword{apa} to a 
%  \dword{felix}  card and  \dword{felix}  host RAM buffer; upon receiving an external trigger, the
% data is propagated to the back-end system. The back-end system
% operates similarly to \dword{dune} itself. What differs in the final \dword{dune}
% implementation is that neither in the host CPU or
% GPU, nor in the added \dword{fpga} functionality does data processing  trigger primitive  generation and subsequent processing
% of \dwords{trigprimitive} through the \dword{daqdsn} system. Another
% significant difference is the much higher rate of data propagation from the
% host RAM to the back-end system in \dword{protodune} than anticipated for the \dword{fd}. Future development
% will concentrate on data processing and \dword{daqdsn}.  This is described in more detail in Section~\ref{sec:sp-daq:validation-pd-demonstrator}.

% \subsubsubsection{Ongoing RCE Throughput Demonstration at \dword{protodune}}
% \label{sec:sp-daq:validation-pdune-rce}
% %\fixme{single-phase module}

% The RCE \dword{daq} in \dword{protodune} demonstrates part of the readout scheme for
% the  \dword{felix} +FPGA readout approach for the TPC. In particular, it
% shows that real-time TPC data processing for lossy
% and lossless compression can be facilitated in FPGA, achieving
% compression factors consistent with those expected based on observed
% \dword{protodune} noise levels. In the future, the system will be used
% to demonstrate how \dwords{trigprimitive} are generated in FPGA as
% described in Section~\ref{sec:sp-daq:validation-pd-demonstrator}.

% \subsubsubsection{Validation of Trigger Primitives in Software}
% \label{sec:sp-daq:validation-software-trigger-primitives}
% %\fixme{single-phase module}

% Generating \dwords{trigprimitive} in CPU or GPU software has not
% yet been demonstrated \textit{in situ} in \dword{protodune}, but it has been
% demonstrated in simulations, using real data from \dword{protodune}, on a
% server with specs similar to those 
% of the  \dword{felix}  host server at \dword{protodune}.

% The algorithm is described in detail in \cite{docid-11236}; a Monte Carlo test has demonstrated the algorithm can do real-time processing of one APAs worth
% of collection wire data. This test was performed on a Xeon Gold 6140
% system, where four (4) threads (cores) were sufficient to keep
% up with the detector data rate. The algorithm begins with pedestal removal by
% finding the mean of any given wire waveform baseline using a frugal
% streaming approach. A look-ahead approach was used to stop
% updating the pedestal when it became clear that a potential signal
% had been found. Noise filtering in the form of a 7-tap low-pass FIR
% filter will be applied as an intermediate step
% between pedestal subtraction and hit finding. The hit finding
% algorithm is a simple threshold-discriminator. Using Monte Carlo, a hit
% threshold of 10 \dword{adc} counts, corresponding to 1\/4 MIP for deposits originating at the cathode, yields hit primitives dominated by
% $^{39}$Ar and maintains 100\% efficiency to MIP hits. This is robust against noise level 
% increases of 50\% above the default \dword{dune} Monte Carlo settings \cite{docid-11275}. 

% Tests of \dword{trigprimitive} generation on very early \dword{protodune} data have also been
% performed. Because of its surface
% location and the known noisy channels included in this study,
% \dword{protodune} \dword{trigprimitive} generation rates level off to a floor rate higher than a threshold
% of approximately 16 \dword{adc} counts. Although the total rates are much higher than
% anticipated for DUNE, the algorithm performance is promising and should improve as \dword{protodune} continues
% to run and improve its noise understanding.

% \subsubsubsection{Validation of Trigger Primitives in Firmware}
% \label{sec:sp-daq:validation-firmware-trigger-primitives}
% %\fixme{single-phase module}

% While \dword{trigprimitive} generation in firmware has not yet been
% demonstrated in the RCE system in \dword{protodune}, candidate
% algorithms are being developed and will be deployed in an \dword{fpga}
% for both \dword{protodune} and other demonstrators. 

% On the other hand, \microboone has been able to
% successfully implement dynamic baseline estimates and subtraction
% for region-of-interest hit finding in an \dword{fpga} \cite{NNN18}.
% %\metainfo{Succinctly describe algorithm, include physics and computing
% %  performance numbers.}

% \subsubsubsection{Validation of Hierarchical Trigger Layers}

% As described in Section~\ref{sec:sp-daq:design-selection-algs}, the \dfirst{daqdss} is structured as a layered hierarchy. 
% It must take in constant, high-rate data and output a minuscule rate that provides summary of activity in the data (trigger commands).
% To do this, it is structured in layers (primitive, candidate, command) and each layer requires certain multiplicity that spans multiple host computers communicating over the local high-speed but commodity network. 
% The algorithms, host computer multiplicity, the \dfirst{ipc} system and its network traffic must be demonstrated at scale.
% This will require development of the following elements:

% \begin{itemize}
% \item A model of \dword{trigprimitive} rates and will be developed that is parameterized by signal, background and noise.  This model will be informed by \dword{protodune} and simulation studies.  The model will include the data density per unit of front-end computing as informed by the \dword{trigprimitive} validation work described above.
% \item Likewise an initial model of trigger candidate rates will be developed and concurrently prototype trigger candidate algorithms will be developed and applied to \dwords{trigprimitive} produced by their validation work (again, described above).
% \item Trigger candidate message schema and IPC application-level protocol (described more below) for their transfer will be developed.
% \item Validation will involve implementing these models, prototype algorithms, schema, etc,  initially at a small-scale on a single high-core computer and using throttled localhost connectivity.
% \item The full-scope system will require \bigo{100} host computers, but contiguous branches of the hierarchy can and will be fully tested with \bigo{20} host computers. 
%   These hosts will support 10 Gbps networking (actual or throttled) to match the expected networking. 
%   In some cases higher bandwidth connectivity may be investigated if bottlenecks are discovered. 
% \end{itemize}

% \subsubsubsection{Validation of High Level Filter in Software}

\fixme{The following needs to be integrated in text above, with
  comparable level of detail}
\subsubsubsection{Prototype Inter-process Communication System}
\label{sec:fd-daq:validation-demonstrators}
\fixme{module-generic}

\fixme{This must be condensed.}

A prototype \dfirst{ipc} system is currently under development. 
Some of the goals of this prototype are:

\begin{itemize}
\item Evaluate raw data throughput, particularly via inter-thread communication transport.
\item Evaluate packet rate limitations, particularly those relevant to the hierarchical trigger layers of the \dword{daqdsn} system described in Section~\ref{sec:sp-daq:design-selection-algs}.
\item Understand the required message schema and protocols.
\item Prototype high-level functionality described in Section~\ref{sec:daq:design-ipc} such as zero-downtime reconfiguration, self healing and the patterns required \dword{daqccm} as described in Section~\ref{sec:fd-daq:design-run-control}.
\item Investigate scaling in terms of performance and software complexity management.
\item Provide functional support for the vertical and horizontal slice tests described above.
\end{itemize}

The prototype software development follows some design principles:

\begin{itemize}
\item The \dword{daq} is modeled as a cyclic, data flow graph.
\item Graph nodes in the graph perform transformations and may consume data input from and produce data output to other nodes.
\item Graph edges connect nodes through ports associated with a network socket.
\item Graph construction emerges by initiating connections locally.
\item Executable processes provide one or more nodes that operate asynchronously from one-another governed only by the flow of messages across their shared edges.
\item Construction of nodes in executable processes and larger graph construction is dynamic governed by initial user configuration and later by messages flowing in the graph itself.
\end{itemize}

The prototype development is based on the well-established, high-quality free software from the ZeroMQ group.
Some of the reasons for selecting their technology include:
\begin{itemize}
\item ZeroMQ software follows a well layered set of software libraries that emphasize portability, high performance, fault tolerance, minimal software dependency and long-lived use and support.
\item A wide variety of language bindings exist, importantly C/C++ for high performance and Python for fast development are two of the best supported.
\item ZeroMQ abstracts functionality in important ways such as concrete implementation of high level communication patterns (eg, pub/sub) and high-level independence from low-level transport mechanism (three are supported: inter-thread queues, inter-process Unix-domain sockets and inter-computer network sockets). 
\item It supports a truly decentralized system design (this is the ``Zero'' in ``ZeroMQ'') critical to satisfying the requirements on robustness.  In particular the ZeroMQ project Zyre provides a distributed, low-latency \dword{daqdispre} system.
\item ZeroMQ has been evaluated favorably CERN~\cite{Dworak:2012mf} and has been used in various \dword{daq} contexts in other experiments.
\end{itemize}

Initial prototype tests have demonstrated that the raw data throughput of ZeroMQ inter-thread transport is sufficient for use in even the highest rate \dword{daq} context (input to \dword{trigprimitive} production).
Rate tests of small packets have been performed to gauge ZeroMQ ability to handle high packet rate \dword{trigprimitive} or trigger candidate data. 
These tests have so far been performed merely on Gbps networks. 
Given reasonably linear scaling as well as benchmarks preformed on faster networks by others, ZeroMQ will perform adequately for DUNE FD \dword{daq} purposes.

The ZeroMQ-based prototype software will continue to be developed with the goal to understand how to manage software and configuration complexity and to participate in the other demonstrators described in this section. 


% \subsubsection{Future \dword{protodune} \dword{daq} Demonstrator}
% \label{sec:sp-daq:validation-pd-demonstrator}

% A demonstration of the \dword{daq} system design based in \dword{protodune} is
% planned for 2020. The key components are the front-end readout, timing
% system, and \dword{daqdsn} systems. A full vertical slice through the
% timing and readout system will be constructed, with
% TPC and PDS detectors, as well as calibration systems. Although a full-scale
% demonstration of the \dword{daqdsn} system cannot be achieved with
% \dword{protodune}, the basic infrastructure will be demonstrated and
% used to self-trigger \dword{protodune}.

% The high-level goals will be to demonstrate and validate
% integration of \dword{fe} with TPC and PDS electronics, 
% data throughput for a single \dword{fe} instance 
% readout data processing for a single \dword{fe} instance 
% \dword{trigprimitive} generation for a single front-end instance 
% distribution of timing and control signals to detector \dwords{fe} 
% data selection infrastructure and self-triggering  and 
% integration with calibration systems.

% The components used in the \dword{protodune}
% demonstrator will be assembled in 2019, and their operation will
% be verified in standalone tests. For \dword{fpga} processing, the
% initial version of firmware will be demonstrated in \dword{fpga} development
% cards, and an integration test will be conducted in 2019 with a
% prototype \dword{fpga} processing module and the  \dword{felix} \dword{fe}.


\subsection{Additional Teststands}
\label{sec:sp-daq:validation-demonstrators}
%\fixme{single-phase module}
%\metainfo{Describe VST demonstrators and why we must build them.}

Concurrently with protodune operation and development, a number of
``vertical slice'' teststands will be built to allow 
development and testing of individual parts of the \dword{daq} system
as well as testing of key aspects of the design and overall
scalability. A data selection subsystem vertical slice teststand will be
constructed and operated on fake-generated data, to assist in the
development of data selection, exercise the system for a variety of
configurations, perform small-scale tests that stress the critical
parts of the corresponding infrastructure, 
and identify likely failure points and/or bottlenecks. The subsystem
will also be deployed and exercised on existing HPC clusters of
comparable resources and specifications as planned for the final
production system for ``horizontal slice'' tests of similar nature. The
back-end DAQ subsystem will be developed and tested in a similar way.

In addition to dedicated vertical and horizontal slice teststands, a number of
\dword{daq} development kits will be available for the consortium for
specific component testing, as well as to other detector and
calibration consortia to support their own development, production, and quality assurance programs. The \dword{daq} 
kit will also form the basis for testing at APA Construction sites beginning in 2020. 

\section{Production, Assembly, Installation and Integration}
\label{sec:sp-daq:production}

(Not finished.)

\subsection{Production and Assembly}
\metainfo{Describe how hardware, firmware and software will produced. }

\subsubsection{Computing Hardware}

\subsubsection{Custom Hardware Fabrication}

\subsubsection{Software and Firmware Development}
\metainfo{Processes and practices.}

\subsection{Installation and Integration}
\fixme{Alec}

\metainfo{Describe how we get stuff in place underground, how we will put it all together and make sure it works. 
  What can we do to minimize the effort needed underground both in
  terms of physical work but also in working out the bugs both in
  individual processes and in emergent behavior of the system as a
  whole?}

\section{Organization and Project Management}

\subsection{Consortium Organization}

The DAQ Consortium was formed in xx as a joint single and
dual phase consortium, with a Consortium Leader and a Technical
Leader. The organization of the consortium is shown in
Figure~\ref{fig:daq-org}. The DAQ consortium board currently comprises
institutional representatives from 30 institutes as shown in Table~\ref{tab:daq-ib}. The consortium leader is the spokesperson for the consortium and
responsible for the overall scientific program and management of the
group. The technical leader of the consortium is responsible for
managing the project for the group.

The consortium's initial mandate has been the design, construction,
and commissioning 
of the DUNE FD DAQ system. To realize this, the consortium was
initially organized in the form of five working groups: (1)
Architecture, (2) Hardware, (3) Data Selection, (4) Back-end DAQ, and (5)
Installation and Infrastructure. This organization has seen the
project through the conceptual design phase.  

A new organizational
structure has been adopted to see the project through engineering
design and construction, and this structure is expected to evolve in
order to meet the needs of the consortium. This is shown in Figure~\ref{fig:daq-org}. Each working group has a designated working group
leader. In addition to the
working group leads, technical design report editors are responsible for the
overall editing and delivery of the TDR document.

\fixme{This is DRAFT/IN DEVELOPMENT. From Giovanna; needs consortium input. }
\begin{dunefigure}{fig:daq-org}{Organizational chart for the \dword{daq} Consortium
 }
  \includegraphics[width=0.8\textwidth]{daq-org.png}
\end{dunefigure}

\fixme{This is DRAFT. Needs consortium vetting. }
\begin{dunetable}
[DAQ Consortium Board institutional members and countries]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:daq-ib}
{DAQ Consortium Board institutional members and countries.}   
Member Institute & County  \\ \toprowrule
CERN & CERN     \\ \colhline
Universidad Sergio Arboleda (USA) & Colombia     \\ \colhline
Lyon & France \\ \colhline
Iwate & Japan     \\ \colhline
KEK & Japan     \\ \colhline
NIT Kure & Japan     \\ \colhline
NIKHEF & Netherlands    \\ \colhline
University of Birmingham & UK     \\ \colhline
Bristol University & UK     \\ \colhline
University of Edinburgh & UK     \\ \colhline
Imperial College London & UK     \\ \colhline
University of Liverpool & UK     \\ \colhline
Oxford University & UK     \\ \colhline
Rutherford Appleton Lab (RAL) & UK     \\ \colhline
University of Sussex Sussex & UK     \\ \colhline
University College London (UCL) & UK     \\ \colhline
University of Warwick & UK     \\ \colhline
Brookhaven National Lab (BNL) & USA     \\ \colhline
Colorado State University (CSU) & USA     \\ \colhline
Columbia University  & USA     \\ \colhline
University of California, Davis (UCD) & USA     \\ \colhline
Duke University & USA     \\ \colhline
University of California, Irvine (UCI) & USA     \\ \colhline
Fermi National Lab (FNAL) & USA     \\ \colhline
Iowa State University & USA     \\ \colhline
University of Minnesota, Duluth (UMD) & USA     \\ \colhline
University of Notre Dame & USA     \\ \colhline
University of Pennsylvania (Penn) & USA     \\ \colhline
Pacific Northwest National Lab (PNNL) & USA     \\ \colhline
South Dakota School of Mines and Technology (SDSMT) & USA     \\ \colhline
Stanford Linear Accelerator Lab (SLAC) & USA     \\ \colhline
\end{dunetable}

\subsection{Cost and Labor}
\label{sec:sp-daq:cost}

\fixme{new standard risks table for autogenerating latex as of 3/25. I will send email. Anne}
\fixme{Table~\ref{tab:Xsched} is a standard table template for the TDR schedules.  It contains overall FD dates from Eric James as of March 2019 (orange) that are held in macros in the common/defs.tex file so that the TDR team can change them if needed. Please do not edit these lines! Please add your milestone dates to fit in with the overall FD schedule. Please set captions and label appropriately. Anne}


Table~\ref{tab:daq-cost} shows the current cost estimates for the DAQ
subsystems major components necessary to serve the first DUNE FD
module. Costs are expected to be reduced for subsequent modules, since
multiple components are common across modules. When appropriate, the quantities of
components are shown, along with the total cost and a brief description of
what is included in the cost estimate. The cost estimates include
materials and supplies (M\&S) for construction, and packing and
shipping to SURF, but not labor and travel costs for construction, or
spares. 

Labor costs depend on personnel category (e.g., faculty, student,
technician, post-doc, engineer), and vary by region and
institution. As such, costs are quantified using labor hours needed to
fulfil a given task. Table~\ref{tab:daq-labor} provides estimates of
labor hours for each subsystem. Signficant physics and simulation
effort is needed in particular for data selection related studies; those
labor resources are listed separately.

\begin{dunetable}
[DAQ System Cost Summary]
{p{0.15\textwidth}p{0.1\textwidth}p{0.15\textwidth}p{0.4\textwidth}}
{tab:daq-cost}
{Cost estimates for different DAQ subsystems. All cost estimates
  include M\&S for construction only. Packing and shipping costs are
  included; spares are not included. }   
System & Quantity & Cost (under development) (k\$ US) & Description \\ \toprowrule
TPC Front-end Readout & 75 & - & Felix and co-processor, host server, and networking  \\ \colhline
PDS Front-end Readout & 6-8 & - & Felix, host server, and networking  \\ \colhline
Low-Level TPC Data Selection & 75 & -&  \\ \colhline
Low-Level PDS Data Selection & 6-8 & -&  \\ \colhline
High-Level PDS Data Selection & 1 & - & MLT, EXT and interface boards, High-Level Filter
networking \\ \colhline
Back-end DAQ & & - & \\ \colhline 
\end{dunetable}

\begin{dunetable}
[DAQ System Labor]
{p{0.25\textwidth}p{0.15\textwidth}p{0.1\textwidth}p{0.08\textwidth}p{0.08\textwidth}p{0.1\textwidth}p{0.08\textwidth}}
{tab:daq-labor}
{Estimate of labor hours for each category of personnel for different DAQ subsystems.}
System  & Faculty/Scientist & Post-doc & Student & Engineer & Technician  &  \textbf{Total}\\ \toprowrule
& (hours) & (hours)& (hours)& (hours)& (hours)& (hours)\\ \toprowrule
Front-end Readout & -& -& -& -& - & - \\ \colhline
Data Selection & -& -& -& -& - & - \\ \colhline
Back-end DAQ & -& -& -& -& - & - \\ \colhline
IPC & -& -& -& -& - & - \\ \colhline
CCM & -& -& -& -& - & - \\ 
Physics \& Simulation & -& -& -& -& - & - \\ \colhline
\end{dunetable}

Following the funding model envisioned for the consortium, various
responsibilities have been distributed across institutions within the
consortium. At this stage of the project, these should be considered
as ``aspirational'' responsibilities until firm funding decisions are
made. Table~\ref{tab:daq-inst-resp} shows the current institutional
responsibilities for primary DAQ subsystems. Only lead institutes are
listed in the table for a given effort. For physics and simulation
studies, and validation efforts at ProtoDUNE, wider institutional effort is
involved. A detailed list of tasks and institutional responsibilities
are presented in \[WBS ref\].

\begin{dunetable}
[DAQ System Institutional Responsibilities]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:daq-inst-resp}
{Institutional responsibilities in the DAQ Consortium}
DAQ Sub-system  & Institutional Responsibility\\ \toprowrule
Front-end Readout & Institutes \\ \colhline
Data Selection & Institutes \\ \colhline
Back-end DAQ & Institutes\\ \colhline
IPC & Institutes \\ \colhline
CCM & Institutes \\ 
Physics \& Simulation & Institutes\\ \colhline
\end{dunetable}

\subsection{Schedule and Milestones}

\begin{dunetable}
[Consortium X Schedule]
{p{0.65\textwidth}p{0.25\textwidth}}
{tab:Xsched}
{Consortium X Schedule}   
Milestone & Date (Month YYYY)   \\ \toprowrule
Technology Decision Dates &      \\ \colhline
Final Design Review Dates &      \\ \colhline
Start of module 0 component production for ProtoDUNE-II &      \\ \colhline
End of module 0 component production for ProtoDUNE-II &      \\ \colhline
\rowcolor{dunepeach} Start of \dword{pdsp}-II installation& \startpduneiispinstall      \\ \colhline
\rowcolor{dunepeach} Start of \dword{pddp}-II installation& \startpduneiidpinstall      \\ \colhline
 \dword{prr} dates &      \\ \colhline
Start of  (component 1) production  &      \\ \colhline
Start of (component 2) production  &      \\ \colhline
Start of  (component 3) production  &      \\ \colhline
\rowcolor{dunepeach}South Dakota Logistics Warehouse available& \sdlwavailable      \\ \colhline
\rowcolor{dunepeach}Beneficial occupancy of cavern 1 and \dword{cuc}& \cucbenocc      \\ \colhline
\rowcolor{dunepeach} \dword{cuc} counting room accessible& \accesscuccountrm      \\ \colhline
\rowcolor{dunepeach}Top of \dword{detmodule} \#1 cryostat accessible& \accesstopfirstcryo      \\ \colhline
End of  (component 1) production  &      \\ \colhline
... & ...                       \\ \colhline
\rowcolor{dunepeach}Start of \dword{detmodule} \#1 TPC installation& \startfirsttpcinstall      \\ \colhline
\rowcolor{dunepeach}End of \dword{detmodule} \#1 TPC installation& \firsttpcinstallend      \\ \colhline
\rowcolor{dunepeach}Top of \dword{detmodule} \#2 accessible& \accesstopsecondcryo      \\ \colhline
 \rowcolor{dunepeach}Start of \dword{detmodule} \#2 TPC installation& \startsecondtpcinstall      \\ \colhline
\rowcolor{dunepeach}End of \dword{detmodule} \#2 TPC installation& \secondtpcinstallend      \\ \colhline
last item & ...                         \\
\end{dunetable}

\subsection{Safety and Risks}

